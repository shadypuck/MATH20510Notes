\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{2}

\begin{document}




\chapter{Multilinear Spaces, Operations, and Conventions}
\section{Exterior Powers Basis and the Determinant}
\begin{itemize}
    \item \marginnote{4/13:}Plan:
    \begin{itemize}
        \item Finish multilinear algebra.
        \item Basis for $\lam[k]{V^*}$.
        \item Talk a bit about pullbacks and the determinant.
        \item \textbf{Orientations} of vector spaces.
        \item The \textbf{interior product}.
    \end{itemize}
    \item Basis for $\lam[k]{V^*}$.
    \begin{itemize}
        \item Recall that $\{\Alt(e_I^*)\mid I\text{ is a nonrepeating, increasing partition of $n$ into $k$ parts}\}$ is a basis for $\alt[k]{V}$.
    \end{itemize}
    \item $\Alt$ is an isomorphism from $\lam[k]{V^*}$ to $\alt[k]{V}$.
    \item If we have an injective map from $\alt[k]{V}$ to $\lin[k]{V}$ and $\pi$ a projection map from $\lin[k]{V}$ to the quotient space $\alt[k]{V^*}$ gives rise to $\pi|_{\alt[k]{V}}$.
    \item Claim:
    \begin{enumerate}
        \item $\pi|_{\alt[k]{V}}$ is an isomorphism.
        \item $\pi(\Alt(e_I^*))=k!\pi(e_I^*)$.
    \end{enumerate}
    (2) implies that $\{\pi(e_I^*)=e_{i_1}^*\wedge\cdots\wedge e_{i_k}^*,\ I\text{ non-repeating and increasing}\}$ is a basis for $\lam[k]{V^*}$.
    \item Examples:
    \begin{enumerate}
        \item $n=2=\dim V$, $V=\R e_1\oplus\R e_2$.
        \begin{itemize}
            \item $\lam[0]{V^*}=\R$ since $\binom{n}{0}=1$.
            \item $\lam[1]{V^*}=\R e_1^*\oplus\R e_2^*$ since $\binom{n}{1}=2$.
            \item $\lam[2]{V^*}=\R e_1^*\wedge e_2^*$ since $\binom{n}{2}=1$.
            \begin{itemize}
                \item Note that $e_1^*\wedge e_2^*=-e_2^*\wedge e_1^*$.
            \end{itemize}
            \item $\lam[3]{V^*}=0$ since $\binom{2}{3}=0$.
            \begin{itemize}
                \item Note that all $e_1^*\wedge e_1^*\wedge e_2^*=0$.
            \end{itemize}
        \end{itemize}
        \item $n=3$, $V=\R e_1\oplus\R e_2\oplus\R e_3$.
        \begin{itemize}
            \item $\binom{n}{0}=1$: $\lam[0]{V^*}=\R$.
            \item $\binom{n}{1}=3$: $\lam[1]{V^*}=\R e_1^*\oplus\R e_2^*\oplus\R e_3^*$.
            \item $\binom{n}{2}=3$: $\lam[2]{V^*}=\R e_1^*\wedge e_2^*\oplus\R e_2^*\wedge e_3^*\oplus\R e_1^*\wedge e_3^*$.
            \item $\binom{n}{3}=1$: $\lam[3]{V^*}=\R e_1^*\wedge e_2^*\wedge e_3^*$.
            \item $\binom{n}{m}=0$ ($m>n$): $\lam[m]{V^*}=\lam[4]{V^*}=0$.
        \end{itemize}
    \end{enumerate}
    \item If $A:V\to W$, $\omega_1\in\lam[k]{W^*}$, $\omega_2\in\lam[\ell]{W^*}$, then
    \begin{equation*}
        A^*(\omega_1\wedge\omega_2) = A^*\omega_1\wedge A^*\omega_2
    \end{equation*}
    \item \textbf{Determinant}: Let $\dim V=n$. Let $A:V\to V$ be a linear transformation. This induces a pullback $A^*:\lam[n]{V^*}\to\lam[n]{V^*}$. The top exterior power $k=n$ implies $\binom{k}{n}=1$. We define $\det(A)$ to be the unique real number such that $A^*(v)=\det(A)v$.
    \item This determinant is the one we know.
    \begin{itemize}
        \item $A^*$ sends $e_1^*\wedge\cdots\wedge e_n^*$ to $A^*e_1^*\wedge\cdots\wedge A^*e_n^*$ which equals $A^*(e_1^*\wedge\cdots\wedge e_n^*)$ or $\det(A)$
    \end{itemize}
    \item Sanity check.
    \begin{enumerate}
        \item $\det(\id)=1$.
        \begin{itemize}
            \item $\id(e_1^*\wedge\cdots\wedge e_n^*)=\id e_1^*\wedge\cdots\wedge\id e_n^*=1\cdot e_1^*\wedge\cdots\wedge e_n^*$.
        \end{itemize}
        \item If $A$ is not an isomorphism, then $\det(A)=0$.
        \begin{itemize}
            \item If $A$ is not an isomorphism, then there exists $v_1\in\ker A$ with $v_1\neq 0$. Let $v_1^*,\dots,v_n^*$ be a basis of $V^*$. So the pullback of this wedge is the wedge of the pullbacks, but $A^*v_1^*=0$, so
            \begin{equation*}
                A^*(v_1^*\wedge\cdots\wedge v_n^*) = (A^*v_1^*)\wedge\cdots\wedge(A^*v_n^*)
                = 0\wedge\cdots\wedge(A^*v_n^*)
                = 0
                = 0\cdot v_1^*\wedge\cdots\wedge v_n^*
            \end{equation*}
        \end{itemize}
        \item $\det(AB)=\det(A)\det(B)$.
        \begin{itemize}
            \item Let $A:V\to V$ and $B:V\to V$.
            \item We have $(AB)^*=B^*A^*$; in particular, $n=k$, $V=W=U=V$.
        \end{itemize}
    \end{enumerate}
    \item Recall: If we pick a basis for $V$, $e_1,\dots,e_n$.
    \begin{itemize}
        \item Implies $[a_{ij}]=[A]_{e_1,\dots,e_n}^{e_1,\dots,e_n}$.
    \end{itemize}
    \item Does $\det(A)=\det([a_{ij}])=\sum_{\sigma\in S_n}(-1)^\sigma a_{1,\sigma(1)}\cdots a_{n,\sigma(n)}$?
    \begin{itemize}
        \item If $A:V\to V$, we know that $A^*:\lam[n]{V^*}\to\lam[n]{V^*}$ takes $e_1^*\wedge\cdots\wedge e_n^*\mapsto A^*(e_1^*\wedge\cdots\wedge e_n^*)$. We WTS
        \begin{equation*}
            A^*(e_1^*\wedge\cdots\wedge e_n^*) = \left[ \sum_{\sigma\in S_n}(-1)^\sigma a_{1,\sigma(1)}\cdots a_{n,\sigma(n)} \right]e_1^*\wedge\cdots\wedge e_n^*
        \end{equation*}
        \item We have that
        \begin{align*}
            A^*(e_1^*\wedge\cdots\wedge e_n^*) &= A^*e_1^*\wedge\cdots\wedge A^*e_n^*\\
            &= \left( \sum_{i_1=1}^na_{i_1,1}e_{i_1}^* \right)\wedge\cdots\wedge\left( \sum_{i_n=1}^na_{i_n,n}e_{i_n}^* \right)\\
            &= \sum_{i_1,\dots,i_n}a_{i_1,1}\cdots a_{i_n,n}e_{i_1}^*\wedge\cdots\wedge e_{i_n}^*\\
            &= \left[ \sum_{\sigma\in S_n}(-1)^\sigma a_{1,\sigma(1)}\cdots a_{n,\sigma(n)} \right]e_1^*\wedge\cdots\wedge e_n^*
        \end{align*}
        where the sign arises from the need to reorder $e_{i_1}^*\wedge\cdots\wedge e_{i_n}^*$ and the antisymmetry of the wedge product.
    \end{itemize}
\end{itemize}



\section{The Interior Product and Orientations}
\begin{itemize}
    \item \marginnote{4/15:}Plan:
    \begin{itemize}
        \item Orientations.
        \item Interior product.
    \end{itemize}
    \item \textbf{Interior product}: We know that $\lam[k]{V^*}\cong\alt[k]{V}$. Fix $v\in V$. Define $\iota_v:\lam[k]{V^*}\to\lam[k-1]{V^*}$.
    \begin{itemize}
        \item Wrong way: We take $\iota_v:\lin[k]{V}\to\lin[k-1]{V}$.
        \begin{equation*}
            T \mapsto \sum_{r=1}^k(-1)^{r-1}T(v_1,\dots,v_{r-1},v,v_r,\dots,v_{k-1})
        \end{equation*}
        \item Right way: First define $\varphi_v:\alt[k]{V}\to\alt[k-1]{V}$ by
        \begin{equation*}
            T \mapsto T_v(v_1,\dots,v_{k-1})=T(v,v_1,\dots,v_{k-1})
        \end{equation*}
        \begin{itemize}
            \item Check:
            \begin{enumerate}
                \item $T_{v_1+v_2}=T_{v_1}+T_{v_2}$\footnote{Should this be $T$ or $\varphi$?}.
                \item $T_{\lambda v}=\lambda T_v$.
                \item $\varphi_v^{k-1}\circ\varphi_v^k=0$ implies $\varphi_v\circ\varphi_w=-\varphi_w\circ\varphi_v$.
            \end{enumerate}
        \end{itemize}
    \end{itemize}
    \item Properties:
    \begin{enumerate}
        \setcounter{enumi}{-1}
        \item $\iota_vT\in\lin[k-1]{V}$.
        \item $\iota_v$ is a linear map.
        \begin{itemize}
            \item This is all happening in the set $\Hom(\lin[k]{V},\lin[k-1]{V})$.
        \end{itemize}
        \item $\iota_{v_1+v_2}=\iota_{v_1}+\iota_{v_2}$; $\iota_{\lambda v}=\lambda\iota_v$.
        \item "Product rule": If $T_1\in\lin[p]{V}$ and $T_1\in\lin[q]{V}$, then $\iota_v(T_1\otimes T_2)=\iota_vT_1\otimes T_2+(-1)^pT_1\otimes\iota_vT_2$.
        \item We have
        \begin{equation*}
            \iota_v(\ell_1\otimes\cdots\otimes\ell_k) = \sum_{r=1}^k(-1)^{r-1}\ell_r(v)\ell_1\otimes\cdots\otimes\hat{\ell}_r\otimes\cdots\otimes\ell_k
        \end{equation*}
        \item $\iota_v\circ\iota_v=0\in\Hom(\lin[k]{V},\lin[k-2]{V})$.
        \begin{itemize}
            \item Note that this is related to $d^2=0$ from the first day of class (alongside $\int_m\dd{w}=\int_{\partial m}w$).
            \item Proof: We induct on $k$. It suffices to prove the result for $T$ decomposable.
            \item Trivial base case for $k=1$.
            \item We have that
            \begin{align*}
                (\iota_v\circ\iota_v)(\ell_1\otimes\cdots\otimes\ell_{k-1}\otimes\ell) &= \iota_v(\iota_vT\otimes\ell+(-1)^{k-1}\ell(v)T)\\
                &= \iota_v(\iota_vT\otimes\ell)+(-1)^{k-1}\ell(v)\iota_vT\\
                &= (-1)^{k-2}\ell(v)\iota_vT+(-1)^{k-1}\ell(v)\iota_vT\\
                &= (-1)^{k-2}\ell(v)\iota_vT-(-1)^{k-2}\ell(v)\iota_vT\\
                &= 0
            \end{align*}
        \end{itemize}
        \item If $T\in\ide[k]{V}$, then $\iota_vT\in\ide[k-1]{V}$.
        \begin{itemize}
            \item Thus, $\iota_v$ induces a map $\iota_v:\lam[k]{V^*}\to\lam[k-1]{V^*}$.
            \item Proof: It suffices to check this for decomposables.
        \end{itemize}
        \item $\iota_{v_1}\circ\iota_{v_2}=-\iota_{v_2}\circ\iota_{v_1}$.
    \end{enumerate}
    \item Orientations.
    \item Motivating example: Investigating a property of bases that we will later formally define as \emph{orientation}.
    \begin{figure}[H]
        \centering
        \footnotesize
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}
                \draw [stealth-stealth] (0,1) node[above]{$e_2$} -- (0,0) -- (1,0) node[right]{$e_1$};
                \draw [rex,thick,-latex] (0:0.4) arc[start angle=0,end angle=90,radius=4mm];
            \end{tikzpicture}
            \caption{The ordering $(e_1,e_2)$.}
            \label{fig:orientationMotivationR2a}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}
                \draw [stealth-stealth] (0,1) node[above]{$e_2$} -- (0,0) -- (1,0) node[right]{$e_1$};
                \draw [rex,thick,-latex] (90:0.4) arc[start angle=90,end angle=0,radius=4mm];
            \end{tikzpicture}
            \caption{The ordering $(e_2,e_1)$.}
            \label{fig:orientationMotivationR2b}
        \end{subfigure}\\[2em]
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}
                \draw [stealth-stealth] (60:1) node[above]{$v_2$} -- (0,0) -- (10:2) node[right]{$v_1$};
                \draw [rex,thick,-latex] (10:0.4) arc[start angle=10,end angle=60,radius=4mm];
            \end{tikzpicture}
            \caption{The ordering $(v_1,v_2)$.}
            \label{fig:orientationMotivationR2c}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}
                \draw [stealth-stealth] (60:1) node[above]{$v_2$} -- (0,0) -- (10:2) node[right]{$v_1$};
                \draw [rex,thick,-latex] (60:0.4) arc[start angle=60,end angle=10,radius=4mm];
            \end{tikzpicture}
            \caption{The ordering $(v_2,v_1)$.}
            \label{fig:orientationMotivationR2d}
        \end{subfigure}
        \caption{Motivating orientations in $\R^2$.}
        \label{fig:orientationMotivationR2}
    \end{figure}
    \begin{itemize}
        \item Consider the vector space $\R^2$.
        \item In both Figures \ref{fig:orientationMotivationR2a} and \ref{fig:orientationMotivationR2b}, $\{e_1,e_2\}$ is the standard basis of $\R^2$. However, suppose we \emph{order} the elements of this basis, i.e., by using a 2-tuple or ordered pair. There are two possible ways we can order the elements: $(e_1,e_2)$ and $(e_2,e_1)$.
        \item Now consider an arbitrary basis $\{v_1,v_2\}$ of $\R^2$. Once again, we have two possible orderings: $(v_1,v_2)$ and $(v_2,v_1)$, as depicted in Figures \ref{fig:orientationMotivationR2c} and \ref{fig:orientationMotivationR2d}, respectively.
        \item What is similar between $(e_1,e_2)$ and $(v_1,v_2)$, and between $(e_2,e_1)$ and $(v_2,v_1)$?
        \begin{itemize}
            \item First off, resist the temptation to regard how the vectors are labeled (i.e., with numerical subscripts) as having any bearing on the properties of the vectors themselves. "Both orientations in the first set are labeled $1,2$, and both orientations in the second set are labeled $2,1$" is not what we're looking for.
            \item What we \emph{are} looking for is the observation that in both Figures \ref{fig:orientationMotivationR2a} and \ref{fig:orientationMotivationR2c}, the second vector in the ordering is positioned counterclockwise relative to the first vector, and in both Figures \ref{fig:orientationMotivationR2b} and \ref{fig:orientationMotivationR2d}, the second vector in the ordering is positioned clockwise relative to the first vector. The red arrows in Figures \ref{fig:orientationMotivationR2a}-\ref{fig:orientationMotivationR2d} indicate this relative positioning.
        \end{itemize}
        \item Now that we've seen the pattern in a few specific examples, it should not be too hard to see that for \emph{any} ordered basis of $\R^2$ the second vector \emph{must} be positioned either counterclockwise or clockwise relative to the first vector.
        \begin{itemize}
            \item In fact, taking this farther, let's ask what would happen if a "basis" did not fit this rule. For example, consider the ordered "basis" $(e_1,e_1)$. Clearly, $e_1$ is not positioned counterclockwise or clockwise to $e_1$, but rather lies directly on top of it. So, the reader might think, we have an exception! But this is not the case because, as the quotation marks around "basis" indicate, $\{e_1,e_1\}$ is \emph{not} a basis of $\R^2$ (for example, $e_2\notin\spn\{e_1,e_1\}$).
            \item Similarly, if our ordered "basis" is like $(e_1,-e_1)$, then we at least have two different basis vectors. However, once again, they are not linearly independent, nor do they span $\R^2$. Therefore, they are not a basis.
        \end{itemize}
        \item It follows that the set of ordered bases of $\R^2$ can be partitioned into two equivalence classes: One containing all ordered bases in which the second vector is positioned counterclockwise relative to the first, and the other containing all ordered bases in which the second vector is positioned clockwise relative to the first. All of this arises naturally; the choice of an \emph{orientation} on $\R^2$ is a man-made designation of one of these equivalence classes as the "positive" one and the other as the "negative" one.
    \end{itemize}
    \item Finding the pattern elsewhere: Three dimensions.
    \begin{figure}[h!]
        \centering
        \footnotesize
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}[z={(5mm,3.85mm)}]
                \draw [-stealth] (0,0,0) -- (1,0,0) node[right]{$e_1$};
                \draw [-stealth] (0,0,0) -- (0,0,1) node[above right=-2pt]{$e_2$};
                \draw [-stealth] (0,0,0) -- (0,1,0) node[above]{$e_3$};
                
                \begin{scope}[
                    plane origin={(0,0.7,0)},
                    plane x={($(1,0,0)+(0,0.7,0)$)},
                    plane y={($(0,0,1)+(0,0.7,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (0:0.2) arc[start angle=0,end angle=340,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0.7,0,0)},
                    plane x={($(0,0,1)+(0.7,0,0)$)},
                    plane y={($(0,1,0)+(0.7,0,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (0:0.2) arc[start angle=0,end angle=340,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0,0,0.7)},
                    plane x={($(0,1,0)+(0,0,0.7)$)},
                    plane y={($(1,0,0)+(0,0,0.7)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (0:0.2) arc[start angle=0,end angle=340,radius=2mm];
                \end{scope}
            \end{tikzpicture}
            \caption{The ordering $(e_1,e_2,e_3)$.}
            \label{fig:orientationMotivationR3a}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}[z={(5mm,3.85mm)}]
                \draw [-stealth] (0,0,0) -- (1,0,0) node[right]{$e_1$};
                \draw [-stealth] (0,0,0) -- (0,0,1) node[above right=-2pt]{$e_2$};
                \draw [-stealth] (0,0,0) -- (0,1,0) node[above]{$e_3$};
                
                \begin{scope}[
                    plane origin={(0,0.7,0)},
                    plane x={($(1,0,0)+(0,0.7,0)$)},
                    plane y={($(0,0,1)+(0,0.7,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (0:0.2) arc[start angle=0,end angle=340,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0.7,0,0)},
                    plane x={($(0,0,1)+(0.7,0,0)$)},
                    plane y={($(0,1,0)+(0.7,0,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (0:0.2) arc[start angle=0,end angle=340,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0,0,0.7)},
                    plane x={($(0,1,0)+(0,0,0.7)$)},
                    plane y={($(1,0,0)+(0,0,0.7)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (0:0.2) arc[start angle=0,end angle=340,radius=2mm];
                \end{scope}
            \end{tikzpicture}
            \caption{The ordering $(e_3,e_1,e_2)$.}
            \label{fig:orientationMotivationR3b}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}[z={(5mm,3.85mm)}]
                \draw [-stealth] (0,0,0) -- (1,0,0) node[right]{$e_1$};
                \draw [-stealth] (0,0,0) -- (0,0,1) node[above right=-2pt]{$e_2$};
                \draw [-stealth] (0,0,0) -- (0,1,0) node[above]{$e_3$};
                
                \begin{scope}[
                    plane origin={(0,0.7,0)},
                    plane x={($(1,0,0)+(0,0.7,0)$)},
                    plane y={($(0,0,1)+(0,0.7,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (0:0.2) arc[start angle=0,end angle=340,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0.7,0,0)},
                    plane x={($(0,0,1)+(0.7,0,0)$)},
                    plane y={($(0,1,0)+(0.7,0,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (0:0.2) arc[start angle=0,end angle=340,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0,0,0.7)},
                    plane x={($(0,1,0)+(0,0,0.7)$)},
                    plane y={($(1,0,0)+(0,0,0.7)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (0:0.2) arc[start angle=0,end angle=340,radius=2mm];
                \end{scope}
            \end{tikzpicture}
            \caption{The ordering $(e_2,e_3,e_1)$.}
            \label{fig:orientationMotivationR3c}
        \end{subfigure}\\[2em]
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}[z={(5mm,3.85mm)}]
                \draw [-stealth] (0,0,0) -- (1,0,0) node[right]{$e_1$};
                \draw [-stealth] (0,0,0) -- (0,0,1) node[above right=-2pt]{$e_2$};
                \draw [-stealth] (0,0,0) -- (0,1,0) node[above]{$e_3$};
                
                \begin{scope}[
                    plane origin={(0,0.7,0)},
                    plane x={($(1,0,0)+(0,0.7,0)$)},
                    plane y={($(0,0,1)+(0,0.7,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (90:0.2) arc[start angle=90,end angle=-250,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0.7,0,0)},
                    plane x={($(0,0,1)+(0.7,0,0)$)},
                    plane y={($(0,1,0)+(0.7,0,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (90:0.2) arc[start angle=90,end angle=-250,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0,0,0.7)},
                    plane x={($(0,1,0)+(0,0,0.7)$)},
                    plane y={($(1,0,0)+(0,0,0.7)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (90:0.2) arc[start angle=90,end angle=-250,radius=2mm];
                \end{scope}
            \end{tikzpicture}
            \caption{The ordering $(e_1,e_3,e_2)$.}
            \label{fig:orientationMotivationR3d}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}[z={(5mm,3.85mm)}]
                \draw [-stealth] (0,0,0) -- (1,0,0) node[right]{$e_1$};
                \draw [-stealth] (0,0,0) -- (0,0,1) node[above right=-2pt]{$e_2$};
                \draw [-stealth] (0,0,0) -- (0,1,0) node[above]{$e_3$};
                
                \begin{scope}[
                    plane origin={(0,0.7,0)},
                    plane x={($(1,0,0)+(0,0.7,0)$)},
                    plane y={($(0,0,1)+(0,0.7,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (90:0.2) arc[start angle=90,end angle=-250,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0.7,0,0)},
                    plane x={($(0,0,1)+(0.7,0,0)$)},
                    plane y={($(0,1,0)+(0.7,0,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (90:0.2) arc[start angle=90,end angle=-250,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0,0,0.7)},
                    plane x={($(0,1,0)+(0,0,0.7)$)},
                    plane y={($(1,0,0)+(0,0,0.7)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (90:0.2) arc[start angle=90,end angle=-250,radius=2mm];
                \end{scope}
            \end{tikzpicture}
            \caption{The ordering $(e_2,e_1,e_3)$.}
            \label{fig:orientationMotivationR3e}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}[z={(5mm,3.85mm)}]
                \draw [-stealth] (0,0,0) -- (1,0,0) node[right]{$e_1$};
                \draw [-stealth] (0,0,0) -- (0,0,1) node[above right=-2pt]{$e_2$};
                \draw [-stealth] (0,0,0) -- (0,1,0) node[above]{$e_3$};
                
                \begin{scope}[
                    plane origin={(0,0.7,0)},
                    plane x={($(1,0,0)+(0,0.7,0)$)},
                    plane y={($(0,0,1)+(0,0.7,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (90:0.2) arc[start angle=90,end angle=-250,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0.7,0,0)},
                    plane x={($(0,0,1)+(0.7,0,0)$)},
                    plane y={($(0,1,0)+(0.7,0,0)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (90:0.2) arc[start angle=90,end angle=-250,radius=2mm];
                \end{scope}
                \begin{scope}[
                    plane origin={(0,0,0.7)},
                    plane x={($(0,1,0)+(0,0,0.7)$)},
                    plane y={($(1,0,0)+(0,0,0.7)$)},
                    canvas is plane
                ]
                    \draw [rex,thick,-latex] (90:0.2) arc[start angle=90,end angle=-250,radius=2mm];
                \end{scope}
            \end{tikzpicture}
            \caption{The ordering $(e_3,e_2,e_1)$.}
            \label{fig:orientationMotivationR3f}
        \end{subfigure}
        \caption{Motivating orientations in $\R^3$.}
        \label{fig:orientationMotivationR3}
    \end{figure}
    \begin{itemize}
        \item Here, we will only consider orderings of standard basis $\{e_1,e_2,e_3\}$ of $\R^3$.
        \item Consider Figure \ref{fig:orientationMotivationR3a}.
        \begin{itemize}
            \item Similarly to in $\R^2$, the positioning of vector 2 relative to vector 1 defines an "orientation," which we take to be about vector 3.
            \begin{itemize}
                \item The direction of this "orientation" is indicated by the red circular arrow wrapping around $e_3$ most of the way along it.
            \end{itemize}
            \item Differently, however, we can define additional "orientations" about vector 1 and vector 2 using, respectively, the position of vector 3 relative to vector 2, and the position of vector 1 relative to vector 3.
            \item Importantly, though, all three of these "orientations" are inextricably linked (note how any time one of them inverts, they all invert), so we can treat these three "orientations" (or any one of them in particular) as one overarching \emph{orientation} on $\R^3$.
        \end{itemize}
        \item Now unlike in $\R^2$, we now have more than 2 ways of ordering $\{e_1,e_2,e_3\}$; in fact, we have $3!=6$ such ways. However, we still only have two distinct orientations: Notice that all of the orientations in Figures \ref{fig:orientationMotivationR3a}-\ref{fig:orientationMotivationR3c} are the same, and all of the orientations in Figures \ref{fig:orientationMotivationR3d}-\ref{fig:orientationMotivationR3f} are the same.
        \begin{itemize}
            \item Thus, we may still partition the set of ordered bases of $\R^3$ into two equivalence classes and arbitrarily label one to be "positive" and the other to be "negative."
        \end{itemize}
    \end{itemize}
    \item Running with it: Higher dimensional generalizations.
    \begin{itemize}
        \item Evidently, the paradigm of orientation became much more complicated from $\R^2$ to $\R^3$. As such, it is reasonable to expect that it will become even more complicated in moving to $\R^4$, let alone $\R^n$, especially since these spaces are not so easily visualizable.
        \begin{itemize}
            \item So how do we proceed? Well, when we've sought to express geometric notions in higher dimensions in the past, linear algebra has been the go-to model. So let's use it once again.
        \end{itemize}
        \item We start with two key observations:
        \begin{itemize}
            \item For any two oriented bases $(v_1,\dots,v_n),(w_1,\dots,w_n)$ in the \emph{same} equivalence class (i.e., both having positive orientation or both having negative orientation), there exists a unique linear transformation $T$ with \emph{positive} determinant such that $w_i=Tv_i$ for $i=1,\dots,n$.
            \item For any two oriented bases $(v_1,\dots,v_n),(w_1,\dots,w_n)$ in \emph{opposite} equivalence classes (i.e., one having positive orientation and the other having negative orientation), there exists a unique linear transformation $T$ with \emph{negative} determinant such that $w_i=Tv_i$ for $i=1,\dots,n$.
        \end{itemize}
        \item Thus, to assign an orientation on $\R^n$, we may take the standard basis in numerical order, i.e., $(e_1,\dots,e_n)$, and define the equivalence class of which it is an element to be the positive one. Then to check the orientation of a given basis $(v_1,\dots,v_n)$, we need only find $T$ and calculate its determinant.
    \end{itemize}
    \item So, what? Why do we need orientations?
    \begin{itemize}
        \item Classical single-variable integration determines the "area under the curve" between $a,b$ by summing the height of infinitely many, infinitely small rectangles. In this picture, we can think of $[a,b]$ as our manifold, moving in the positive direction along the $x$-axis as our orientation, and $f$ as a function on the manifold.
        \item For a more general manifold $X$, we will have a \emph{form} (as opposed to a function) which assigns to each $p\in X$ an exterior product/alternating tensor (as opposed to a scalar). But we will still need to know in which direction we should move along the manifold, and that is the job of the orientation.
        \item Additionally, we need a rigorous algebraic theory so that we can relate the orientations of domains to the orientations of their boundaries.
    \end{itemize}
    \item Formally defining orientations: Relating orientation to the language of differential forms.
    \begin{itemize}
        \item It is easy to define an orientation on $\R^1$ or any other 1-dimensional vector space. Indeed, we can take $e_1$ and all of its positive scalar multiples to be the positive equivalence class, and all negative multiples of $e_1$ to be the negative equivalence class.
        \begin{itemize}
            \item Note that since $1\times 1$ matrices have determinant equal to their one entry and, for all intents and purposes, function as scalars, this definition agrees with our previous ones.
            \item Also, an alternate but equivalent definition of orientation on a 1-dimensional vector space is to divide it into two connected components by removing the zero element from the set and choose one of those connected components to be the "positive orientation." This is the definition we will use going forward.
        \end{itemize}
        \item Let $V$ be an arbitrary $n$-dimensional vector space. Since $\dim\lam[n]{V^*}=\binom{n}{n}=1$, we can assign it an orientation based on the above rule.
        \begin{itemize}
            \item Now let $\omega\in\lam[n]{V^*}$ lie in the positive component. Then we say that an ordered basis $(v_1,\dots,v_n)$ of $V$ is positive if $\omega(v_1,\dots,v_n)>0$, and vice versa if $\omega(v_1,\dots,v_n)<0$.
        \end{itemize}
        \item In particular, let $\{e_1,\dots,e_n\}$ be the standard basis of $V$. Then we call $e_1^*\wedge\cdots\wedge e_n^*$ the \textbf{orientation form}.
        \begin{itemize}
            \item It has the ability to identify the standard basis as positive and some others as positive or negative, but also equals zero for a number of bases.
            \item A more powerful tool is $\omega=\Alt(e_1^*\wedge\cdots\wedge e_n^*)\in\lam[n]{V^*}$. We can define an orientation on $\lam[n]{V^*}$ based on which half of it $\omega$ lies in. Moreover, since $\omega$ will take in any ordered basis of $V$ and return a nonzero scalar, it can provide an orientation on $V$.
        \end{itemize}
        \item \emph{Come back after completing integration on manifolds section.}
    \end{itemize}
    \item An interesting connection between the differential forms definition and the determinant definition, and its formalization as motivation for the new definition of the determinant given in class yesterday.
    \begin{itemize}
        \item $\omega$ and $\det(T)$ may seem like entirely different objects at first glance, but in fact, they are almost identical.
        \item Let's drop back down to $\R^2$ to illustrate the point. Here,
        \begin{equation*}
            \omega = \Alt(e_1^*\wedge e_2^*) = e_1^*\wedge e_2^*-e_2^*\wedge e_1^*
        \end{equation*}
        \begin{itemize}
            \item It follows that if
            \begin{equation*}
                \left\{
                    \begin{bmatrix}
                        v_1\\
                        v_2\\
                    \end{bmatrix},
                    \begin{bmatrix}
                        w_1\\
                        w_2\\
                    \end{bmatrix}
                \right\}
            \end{equation*}
            is an arbitrary basis of $\R^2$, then the transformation matrix $T$ from positively-oriented ordered basis $(e_1,e_2)$ to $(v,w)$ is
            \begin{equation*}
                T =
                \begin{bmatrix}
                    v_1 & w_1\\
                    v_2 & w_2\\
                \end{bmatrix}
            \end{equation*}
            which has determinant
            \begin{equation*}
                \det(T) = v_1w_2-w_1v_2
            \end{equation*}
            Therefore, whether $v_1w_2-w_1v_2$ is positive or negative determines whether $(v,w)$ is a positively or negatively oriented basis of $\R^2$.
            \item Incidentally,
            \begin{align*}
                \omega(v,w) &= [e_1^*\wedge e_2^*-e_2^*\wedge e_1^*](v,w)\\
                &= v_1\cdot w_2-v_2\cdot w_1\\
                &= v_1w_2-w_1v_2
            \end{align*}
            as well.
            \item Therefore, evaluating the sign of $\det(T)$ is identical to evaluating the sign of $\omega(v,w)$.
        \end{itemize}
        \item But why the relationship?
        \begin{itemize}
            \item In fact, it's in the definition of the determinant from last class: The determinant of an endomorphism $T:V\to V$ can be interpreted as the induced action of the pullback on the top exterior power.
            \item In particular,
            \begin{align*}
                \omega(v_1,\dots,v_n) &= \omega(T(e_1),\dots,T(e_n))\\
                &= [T^*\omega](e_1,\dots,e_n)\\
                &= \det(T)\cdot\omega(e_1,\dots,e_n)\\
                &= \det(T)\cdot 1\\
                &= \det(T)
            \end{align*}
            as desired.
        \end{itemize}
    \end{itemize}
    \item Summary of orientations.
    \begin{itemize}
        \item A vector space $V$ should have two orientations.
        \item Two bases $e_1,\dots,e_n$ and $f_1,\dots,f_n$ are \textbf{orientation equivalent} if $T:V\to V$ an isomorphism has positive determinant. Otherwise, they are \textbf{orientation inequivalent}.
        \item An orientation on $V$ is a choice of equivalence classes of bases under the equivalence relation on bases.
        \item $T:V\to W$ given orientations, $T$ preserves or reverses orientations.
    \end{itemize}
    \item Fancy orientations.
    \begin{itemize}
        \item An orientation on a 1D vector space $L$ is a division into two halves.
        \item Def: An orientation of $V$ is an orientation of $\lam[n]{V^*}$.
    \end{itemize}
    \item We can prove that they're both the same.
    \begin{itemize}
        \item If $W$ and $V$ are both oriented, then $V/W$ gets a canonical orientation.
    \end{itemize}
\end{itemize}



\section{Chapter 1: Multilinear Algebra}
\emph{From \textcite{bib:DifferentialForms}.}
\begin{itemize}
    \item \marginnote{4/14:}$\bm{\iota_vT}$: The $(k-1)$-tensor defined as follows. \emph{Given by}
    \begin{equation*}
        (v_1,\dots,v_{k-1}) \mapsto \sum_{r=1}^k(-1)^{r-1}T(v_1,\dots,v_{r-1},v,v_r,\dots,v_{k-1})
    \end{equation*}
    where $T\in\lin[k]{V}$, $k\in\N_0$, $V$ is a vector space, and $v\in V$.
    \item If $v=v_1+v_2$, then
    \begin{equation*}
        \iota_vT = \iota_{v_1}T+\iota_{v_2}T
    \end{equation*}
    \item If $T=T_1+T_2$, then
    \begin{equation*}
        \iota_vT = \iota_vT_1+\iota_vT_2
    \end{equation*}
    \item Lemma 1.7.4: If $T=\ell_1\otimes\cdots\otimes\ell_k$, then
    \begin{equation*}
        \iota_vT = \sum_{r=1}^k(-1)^{r-1}\ell_r(v)\ell_1\otimes\cdots\otimes\hat{\ell}_r\otimes\cdots\otimes\ell_k
    \end{equation*}
    where the hat over $\ell_r$ means that $\ell_r$ is deleted from the tensor product.
    \item Lemma 1.7.6: $T_1\in\lin[p]{V}$ and $T_2\in\lin[q]{V}$ imply
    \begin{equation*}
        \iota_v(T_1\otimes T_2) = \iota_vT_1\otimes T_2+(-1)^pT_1\otimes\iota_vT_2
    \end{equation*}
    \item Lemma 1.7.8: $T\in\lin[k]{V}$ implies that for all $v\in V$, we have
    \begin{equation*}
        \iota_v(\iota_vT) = 0
    \end{equation*}
    \begin{proof}
        It suffices by linearity to prove this for decomposable tensors. We induct on $k$. For the base case $k=1$, the claim is trivially true. Now suppose inductively that we have proven the claim for $k-1$. Consider $\ell_1\otimes\cdots\otimes\ell_k$. Taking $T=\ell_1\otimes\cdots\otimes\ell_{k-1}$ and $\ell=\ell_k$, we obtain
        \begin{equation*}
            \iota_v(\iota_v(T\otimes\ell)) = \iota_v(\iota_vT)\otimes\ell+(-1)^{k-2}\ell(v)\iota_vT+(-1)^{k-1}\ell(v)\iota_vT
        \end{equation*}
        The first term is zero by the inductive hypothesis, and the second two cancel each other out, as desired.
    \end{proof}
    \item Claim 1.7.10: For all $v_1,v_2\in V$, we have that
    \begin{equation*}
        \iota_{v_1}\iota_{v_2} = -\iota_{v_2}\iota_{v_1}
    \end{equation*}
    \begin{proof}
        Let $v=v_1+v_2$. Then $\iota_v=\iota_{v_1}+\iota_{v_2}$. Therefore,
        \begin{align*}
            0 &= \iota_v\iota_v\tag*{Lemma 1.7.8}\\
            &= (\iota_{v_1}+\iota_{v_2})(\iota_{v_1}+\iota_{v_2})\\
            &= \iota_{v_1}\iota_{v_1}+\iota_{v_1}\iota_{v_2}+\iota_{v_2}\iota_{v_1}+\iota_{v_2}\iota_{v_2}\\
            &= \iota_{v_1}\iota_{v_2}+\iota_{v_2}\iota_{v_1}\tag*{Lemma 1.7.8}
        \end{align*}
        yielding the desired result.
    \end{proof}
    \item Lemma 1.7.11: If $T\in\lin[k]{V}$ is redundant, then so is $\iota_vT$.
    \begin{proof}
        Let $T=T_1\otimes\ell\otimes\ell\otimes T_2$ where $\ell\in V^*$, $T_1\in\lin[p]{V}$, and $T_2\in\lin[q]{V}$. By Lemma 1.7.6, we have that
        \begin{equation*}
            \iota_vT = \iota_vT_1\otimes\ell\otimes\ell\otimes T_2+(-1)^pT_1\otimes\iota_v(\ell\otimes\ell)\otimes T_2+(-1)^{p+2}T_1\otimes\ell\otimes\ell\otimes\iota_vT_2
        \end{equation*}
        Thus, since the first and third terms above are redundant and $\iota_v(\ell\otimes\ell)=\ell(v)\ell-\ell(v)\ell=0$ by Lemma 1.7.4, we have the desired result.
    \end{proof}
    \item $\bm{\iota_v\omega}$: The $\ide[k]{V}$-coset $\pi(\iota_vT)$, where $\omega=\pi(T)$.
    \item Proves that $\iota_v\omega$ does not depend on the choice of $T$.
    \item \textbf{Interior product operation}: The linear map $\iota_v:\lam[k]{V^*}\to\lam[k-1]{V^*}$.
    \item The interior product has the following important identities.
    \begin{align*}
        \iota_{(v_1+v_2)}\omega &= \iota_{v_1}\omega+\iota_{v_2}\omega\\
        \iota_v(\omega_1\wedge\omega_2) &= \iota_v\omega_1\wedge\omega_2+(-1)^p\omega_1\wedge\omega_2\\
        \iota_v(\iota_v\omega) &= 0\\
        \iota_{v_1}\iota_{v_2}\omega &= -\iota_{v_2}\iota_{v_1}\omega
    \end{align*}
    \item \marginnote{4/18:}As we developed the pullback $A^*T\in\lin[k]{V}$, we now look to develop a pullback on $\lam[k]{V^*}$.
    \item Lemma 1.8.1: If $T\in\ide[k]{W}$, then $A^*T\in\ide[k]{V}$.
    \begin{proof}
        It suffices to prove this for redundant $k$-tensors. Let $T=\ell_1\otimes\cdots\otimes\ell_k$ be such that $\ell_i=\ell_{i+1}$. Then we have that
        \begin{align*}
            A^*T &= A^*(\ell_1\otimes\cdots\otimes\ell_k)\\
            &= A^*\ell_1\otimes\cdots\otimes A^*\ell_k\tag*{Exercise 1.3.iii}
        \end{align*}
        where $A^*\ell_i=A^*\ell_{i+1}$ so that $A^*T\in\ide[k]{V}$, as desired.
    \end{proof}
    \item $\bm{A^*\omega}$: The $\ide[k]{W}$-coset $\pi(A^*T)$, where $\omega=\pi(T)$.
    \item Claim 1.8.3: $A^*\omega$ is well-defined.
    \begin{proof}
        Suppose $\omega=\pi(T)=\pi(T')$. Then $T=T'+S$ where $S\in\ide[k]{W}$. It follows that $A^*T=A^*T'+A^*S$, but since $A^*S\in\ide[k]{V}$ (Lemma 1.8.1), we have that
        \begin{equation*}
            \pi(A^*T) = \pi(A^*T')
        \end{equation*}
        as desired.
    \end{proof}
    \item Proposition 1.8.4. The map $A^*:\lam[k]{W^*}\to\lam[k]{V^*}$ sending $\omega\mapsto A^*\omega$ is linear. Moreover,
    \begin{enumerate}
        \item If $\omega_i\in\lam[k_i]{W^*}$ ($i=1,2$), then
        \begin{equation*}
            A^*(\omega_1\wedge\omega_2) = A^*(\omega_1)\wedge A^*(\omega_2)
        \end{equation*}
        \item If $U$ is a vector space and $B:U\to V$ is a linear map, then for $\omega\in\lam[k]{W^*}$,
        \begin{equation*}
            B^*A^*\omega = (AB)^*\omega
        \end{equation*}
    \end{enumerate}
    (Hint: This proposition follows immediately from Exercises 1.3.iii-1.3.iv.)
    \item \textbf{Determinant} (of $A$): The number $a$ such that $A^*\omega=a\omega$, where $A^*:\lam[n]{V^*}\to\lam[n]{V^*}$. \emph{Denoted by} $\textbf{det}\,\bm{(A)}$.
    \item Proposition 1.8.7: If $A$ and $B$ are linear mappings of $V$ into $V$, then
    \begin{equation*}
        \det(AB) = \det(A)\det(B)
    \end{equation*}
    \begin{proof}
        Proposition 1.8.4(2) implies that
        \begin{align*}
            \det(AB)\omega &= (AB)^*\omega\\
            &= B^*(A^*\omega)\\
            &= \det(B)A^*\omega\\
            &= \det(B)\det(A)\omega
        \end{align*}
        as desired.
    \end{proof}
    \item $\textbf{id}_{\bm{V}}$: The identity map on $V$.
    \item Proposition 1.8.8: $\det(\id_V)=1$.
    \begin{itemize}
        \item Hint: $\id_V^*$ is the identity map on $\lam[n]{V^*}$.
    \end{itemize}
    \item Proposition 1.8.9: If $A:V\to V$ is not surjective, then $\det(A)=0$.
    \begin{proof}
        Let $W=\im(A)$. If $A$ is not onto, $\dim W<n$, implying that $\lam[n]{W^*}=0$. Now let $A=\imath_WB$ where $\imath_W$ is the inclusion map of $W$ into $V$ and $B$ is the mapping $A$ regarded as a mapping from $V$ to $W$. It follows by Proposition 1.8.4(1) that if $\omega\in\lam[n]{V^*}$, then
        \begin{equation*}
            A^*\omega = B^*\imath_W^*\omega
        \end{equation*}
        where $\imath_W^*\omega=0$ as an element of $\lam[n]{W^*}$.
    \end{proof}
    \item Deriving the typical formula for the determinant.
    \begin{itemize}
        \item Let $V,W$ be $n$-dimensional vector spaces with respective bases $e_1,\dots,e_n$ and $f_1,\dots,f_n$.
        \item Denote the corresponding dual bases by $e_1^*,\dots,e_n^*$ and $f_1^*,\dots,f_n^*$.
        \item Let $A:V\to W$. Recall that if the matrix of $A$ is $[a_{i,j}]$, then the matrix of $A^*:W^*\to V^*$ is $(a_{j,i})$, i.e., if
        \begin{equation*}
            Ae_j = \sum_{i=1}^na_{i,j}f_i
        \end{equation*}
        then
        \begin{equation*}
            A^*f_j^* = \sum_{i=1}^na_{j,i}e_i^*
        \end{equation*}
        \item It follows that
        \begin{align*}
            A^*(f_1^*\wedge\cdots\wedge f_n^*) &= A^*f_1^*\wedge\cdots\wedge A^*f_n^*\\
            &= \sum_{1\leq k_1,\dots,k_n\leq n}(a_{1,k_1}e_{k_1}^*)\wedge\cdots\wedge(a_{n,k_n}e_{k_n}^*)\\
            &= \sum_{1\leq k_1,\dots,k_n\leq n}a_{1,k_1}\cdots a_{n,k_n}e_{k_1}^*\wedge\cdots\wedge e_{k_n}^*
        \end{align*}
        \item At this point, we are summing over all possible lists of length $n$ containing the numbers between 1 and $n$ at each index.
        \begin{itemize}
            \item However, any list in which a number repeats will lead to a wedge product of a linear functional with itself, making that term equal to zero.
            \item Thus, it is only necessary to sum over those terms that are non-repeating.
            \item But the terms that are non repeating are exactly the permutations $\sigma\in S_n$.
        \end{itemize}
        \item Thus,
        \begin{align*}
            A^*(f_1^*\wedge\cdots\wedge f_n^*) &= \sum_{\sigma\in S_n}a_{1,\sigma(1)}\cdots a_{n,\sigma(n)}(e_1^*\wedge\cdots\wedge e_n^*)^\sigma\\
            &= \sum_{\sigma\in S_n}(-1)^\sigma a_{1,\sigma(1)}\cdots a_{n,\sigma(n)}e_1^*\wedge\cdots\wedge e_n^*\\
            &= \det([a_{i,j}])e_1^*\wedge\cdots\wedge e_n^*
        \end{align*}
        \item If $V=W$ and $e_i=f_i$ ($i=1,\dots,n$), then we may define $\omega=e_1^*\wedge\cdots\wedge e_n^*=f_1^*\wedge\cdots\wedge f_n^*\in\lam[k]{V^*}$ to obtain
        \begin{align*}
            A^*\omega &= \det([a_{i,j}])\omega
        \end{align*}
        which proves that
        \begin{equation*}
            \det(A) = \det([a_{i,j}])
        \end{equation*}
        as desired.
    \end{itemize}
    \item \textbf{Orientation} (of $\ell$): A choice of one of the disconnected components of $\ell\setminus\{0\}$, where $\ell\subset\R^2$ is a straight line through the origin.
    \item \textbf{Orientation} (of $L$): A choice of one of the connected components of $L\setminus\{0\}$, where $L$ is a one-dimensional vector space.
    \item \textbf{Positive component} (of $L\setminus\{0\}$): The component chosen in the orientation of $L$. \emph{Denoted by} $\bm{L_+}$.
    \item \textbf{Negative component} (of $L\setminus\{0\}$): The component chosen in the orientation of $L$. \emph{Denoted by} $\bm{L_-}$.
    \item \textbf{Positively oriented} ($v\in L$): A vector $v\in L$ such that $v\in L_+$.
    \item \textbf{Orientation} (of $V$) An orientation of the one-dimensional vector space $\lam[n]{V^*}$, where $V$ is an $n$-dimensional vector space.
    \item "One important way of assigning an orientation to $V$ is to choose a basis $e_1,\dots,e_n$ of $V$. Then if $e_1^*,\dots,e_n^*$ is the dual basis, we can orient $\lam[n]{V^*}$ by requiring that $e_1^*\wedge\cdots\wedge e_n^*$ be in the positive component of $\lam[n]{V^*}$" \parencite[29]{bib:DifferentialForms}.
    \item \textbf{Positively oriented} (ordered basis $e_1,\dots,e_n$ of $V$): An ordered basis $e_1,\dots,e_n\in V$ such that $e_1^*\wedge\cdots\wedge e_n^*\in\lam[n]{V^*}_+$.
    \item Proposition 1.9.7: If $e_1,\dots,e_n$ is positively oriented, then $f_1,\dots,f_n$ is positively oriented iff $\det[a_{i,j}]>0$ where $e_j=\sum_{i=1}^na_{i,j}f_i$.
    \begin{proof}
        We have that
        \begin{equation*}
            f_1^*\wedge\cdots\wedge f_n^* = \det[a_{i,j}]e_1^*\wedge\cdots\wedge e_n^*
        \end{equation*}
    \end{proof}
    \item Corollary 1.9.8: If $e_1,\dots,e_n$ is a positively oriented basis of $V$, then the basis
    \begin{equation*}
        e_1,\dots,e_{i-1},-e_i,e_{i+1},\dots,e_n
    \end{equation*}
    is negatively oriented.
    \item Theorem 1.9.9: Given orientations on $V$ and $V/W$ (where $\dim V=n>1$, $W\leq V$, and $\dim W=k<n$), one gets from these orientations a natural orientation on $W$.
    \begin{proof}
        The orientations on $V$ and $V/W$ come prepackaged with a basis. We first apply an orientation to $W$ based on these bases, and then show that any choice of basis for $V,V/W$ induces a basis with the same orientation on $W$. Let's begin.\par
        Let $r=n-k$, and let $\pi:V\to V/W$. By Exercises 1.2.i and 1.2.ii, we may choose a basis $e_1,\dots,e_n$ of $V$ such that $e_{r+1},\dots,e_n$ is a basis of $W$. It follows that $\pi(e_1),\dots,\pi(e_r)$ is a basis of $V/W$. WLOG\footnote{If the first basis is negatively oriented, we may substitute $-e_1$ for $e_1$. If the second basis is negatively oriented, we may substitute $-e_n$ for $e_n$.}, take $\pi(e_1),\dots,\pi(e_r)$ and $e_1,\dots,e_n$ to be positively oriented on $V/W$ and $V$, respectively. Assign to $W$ the orientation associated with $e_{r+1},\dots,e_n$.\par
        Now suppose $f_1,\dots,f_n$ is another basis of $V$ such that $f_{r+1},\dots,f_n$ is a basis of $W$. Let $A=[a_{i,j}]$ express $e_1,\dots,e_n$ as linear combinations of $f_1,\dots,f_n$, i.e., let
        \begin{equation*}
            e_j = \sum_{i=1}^na_{i,j}f_i
        \end{equation*}
        for all $j=1,\dots,n$. Now as will be explained below, $A$ must have the form
        \begin{equation*}
            A =
            \begin{pmatrix}
                B & C\\
                0 & D\\
            \end{pmatrix}
        \end{equation*}
        where $B$ is the $r\times r$ matrix expressing $\pi(e_1),\dots,\pi(e_r)$ as linear combinations of $\pi(f_1),\dots,\pi(f_r)$, and $D$ is the $k\times k$ matrix expressing the basis vectors $e_{r+1},\dots,e_n$ as linear combinations of $f_{r+1},\dots,f_n$. We have just explained $B$ and $D$. We don't particularly care about $C$ or have a good way of defining its structure. We can, however, take the block labeled zero to be the $k\times r$ zero matrix by Proposition 1.2.9; in particular, since these components of these vectors will be fed into $\pi$ and fall within $W$, they can moved around wherever without altering the identities of the $W$-cosets to which they pertain. Having justified this structure for $A$, we see that we can take
        \begin{equation*}
            \det(A) = \det(B)\det(D)
        \end{equation*}
        It follows by Proposition 1.9.7 as well as the positivity of $\det(A)$ and $\det(B)$ that $\det(D)$ is positive, and hence the orientation of $e_{r+1},\dots,e_n$ and $f_{r+1},\dots,f_n$ are one and the same.
    \end{proof}
    \item \textbf{Orientation preserving} (map $A$): A bijective linear map $A:V_1\to V_2$, where $V_1,V_2$ are oriented $n$-dimensional vector spaces, such that for all $\omega\in\lam[n]{V_2^*}_+$, we have that $A^*\omega\in\lam[n]{V_1^*}_+$.
    \item If $V_1=V_2$, $A$ is orientation preserving iff $\det(A)>0$.
    \item Proposition 1.9.14: Let $V_1,V_2,V_3$ be oriented $n$-dimensional vector spaces, and let $A_1:V_1\to V_2$ and $A_2:V_2\to V_3$ be bijective linear maps. Then if $A_1,A_2$ are orientation preserving, so is $A_2\circ A_1$.
\end{itemize}




\end{document}