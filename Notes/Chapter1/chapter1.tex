\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}

\begin{document}




\chapter{Multilinear Algebra}
\section{Notes}
\begin{itemize}
    \item \marginnote{3/28:}Motivation for the course and an overview of \textcite{bib:DifferentialForms}.
    \item \marginnote{3/30:}Plan:
    \begin{itemize}
        \item More (multi)linear algebra.
    \end{itemize}
    \item Dual spaces.
    \item Let $V$ be an $n$-dimensional real vector space.
    \item $\bm{\Hom(V,\pmb{\R})}$: The set of all homomorphisms (i.e., linear maps) from $V$ to $\R$. \emph{Also known as} $\bm{V^*}$.
    \item \textbf{Dual basis} (for $V^*$): The set of linear transformations from $V$ to $\R$ defined by
    \begin{equation*}
        \vec{e}_j \mapsto
        \begin{cases}
            1 & j=i\\
            0 & j\neq i
        \end{cases}
    \end{equation*}
    where $\vec{e}_1,\dots,\vec{e}_n$ is a basis of $V$. \emph{Denoted by} $\vec{e}_1^*,\dots,\vec{e}_n^*$.
    \item Check: $\vec{e}_1^*,\dots,\vec{e}_n^*$ are a basis for $V^*$.
    \begin{itemize}
        \item Are they linearly independent? Let $c_1\vec{e}_1^*+\cdots+c_n\vec{e}_n^*=0\in\Hom(V,\R)$. Then
        \begin{equation*}
            c_i = (c_1\vec{e}_1^*+\cdots+c_n\vec{e}_n^*)(\vec{e}_i) = 0\in\R
        \end{equation*}
        as desired.
        \item Span? Let $\varphi\in\Hom(V,\R)$. Then we can verify that
        \begin{equation*}
            \varphi(\vec{e}_1)\vec{e}_1^*+\cdots+\varphi(\vec{e}_n)\vec{e}_n^* = \varphi
        \end{equation*}
        \begin{itemize}
            \item We prove this by verifying the previous statement on the basis of $V$ (if two linear transformations have the same action on the basis of a vector space, they are equal).
        \end{itemize}
    \end{itemize}
    \item With a choice of basis for $V$, we obtain an isomorphism $\varepsilon:V\to V^*$ with the mapping $\vec{e}_i\mapsto\vec{e}_i^*$ for all $i$.
    \item The dual space is known as such because $(V^*)^*\cong V$, where $\cong$ is \textbf{canonical} (no choice of basis is needed).
    \item One more property of dual spaces: \textbf{functoriality}.
    \begin{itemize}
        \item Given a linear transformation $A:V\to W$, we know that $A^*:W^*\to V^*$ where $A^*$ is the transpose of $A$. In particular, if $\varphi\in W^*$, then $\varphi\circ A:V\to\R$.
        \item Claim: $A^*$ is linear.
    \end{itemize}
    \item \textbf{Functoriality}: If $A:V\to W$ and $B:W\to U$, then $B^*:U^*\to W^*$ and $A^*:W^*\to V^*$. The functoriality statement is that $(B\circ A)^*=A^*\circ B^*$.
    \item $A^*$ is the \textbf{pullback} (or transpose) of $A$.
    \item Let $\vec{v}_1,\dots,\vec{v}_n$ be a basis for $V$ and $\vec{w}_1,\dots,\vec{w}_m$ be a basis for $W$. Then $[A]_{\vec{v}_1,\dots,\vec{v}_n}^{\vec{w}_1,\dots,\vec{w}_m}=A$ is the matrix of the linear transformation $A$ with respect to these bases. Then if $\vec{v}_1^*,\dots,\vec{v}_n^*$ and $\vec{w}_1^*,\dots,\vec{w}_m^*$ are the corresponding dual bases, then $[A^*]_{\vec{v}_1^*,\dots,\vec{v}_n^*}^{\vec{w}_1^*,\dots,\vec{w}_m^*}=A^T$. We can and should verify this for ourselves.
    \item This is over the real numbers, so $A^*$ is just the transpose because there are no complex numbers of which to take the conjugate!
    \item A generalization: Tensors.
    \item \textbf{$\bm{k}$-tensor}: A \textbf{multilinear} map
    \begin{equation*}
        T:\underbrace{V\times\cdots\times V}_{k\text{ times}}\to\R
    \end{equation*}
    \item \textbf{Multilinear} (map $T$): A function $T$ such that
    \begin{align*}
        T(\vec{v}_1,\dots,\vec{v}_i^1+\vec{v}_i^2,\dots,\vec{v}_k) &= T(\vec{v}_1,\dots,\vec{v}_i^1,\dots,\vec{v}_k)+T(\vec{v}_1,\dots,\vec{v}_i^2,\dots,\vec{v}_k)\\
        T(\vec{v}_1,\dots,\lambda \vec{v}_i,\dots,\vec{v}_k) &= \lambda T(\vec{v}_1,\dots,\vec{v}_i,\dots,\vec{v}_k)
    \end{align*}
    for all $(\vec{v}_1,\dots,\vec{v}_k)\in V^k$.
    \item The determinant is an $n$-tensor!
    \item 1-tensors are just covectors.
    \item $\bm{\lin[k]{V}}$: The vector space of all $k$-tensors on $V$.
    \item Calculating $\dim \lin[k]{V}$. (Answer not given in this class.)
    \item Let $A:V\to W$. Then $A^*:\lin[k]{W}\to\lin[k]{V}$.
    \begin{itemize}
        \item Check $(A\circ B)^*=B^*\circ A^*$.
    \end{itemize}
    \item \textbf{Multi-index of $\bm{n}$ of length $\bm{k}$}: A $k$-tuple $(i_1,\dots,i_k)$ where each $i_j\in\N$ satisfies $1\leq i_j\leq n$ ($j=1,\dots,k$). \emph{Denoted by} $\bm{I}$.
    \item Let $\vec{e}_1,\dots,\vec{e}_n$ be a basis for $V$.
    \item \textbf{Tensor product} (of $T_1\in \lin[k]{V}$, $T_2\in L^l(V)$): The function from $V^{k+l}$ to $\R$ defined by
    \begin{equation*}
        (\vec{v}_1,\dots,\vec{v}_{k+l}) \mapsto T_1(\vec{v}_1,\dots,\vec{v}_k)T_2(\vec{v}_{k+1},\dots,\vec{v}_{k+l})
    \end{equation*}
    \emph{Denoted by} $\bm{T_1\otimes T_2}$.
    \item Claims:
    \begin{enumerate}
        \item $T_1\otimes T_2\in L^{k+l}(V)$.
        \item $A^*(T_1\otimes T_2)=A^*(T_1)\otimes A^*(T_2)$.
    \end{enumerate}
    \item $\bm{\vec{e}_I^*}$: The function $\vec{e}_{i_1}^*\otimes\cdots\otimes\vec{e}_{i_k}^*$, where $I=(i_1,\dots,i_k)$ is a multi-index of $n$ of length $k$.
    \item Claim: Letting $I$ range over all $n^k$ multi-indices of $n$ of length $k$, the $\vec{e}_I^*$ are a basis for $\lin[k]{V}$.
    \item If $V=\R$, then $V=\R\vec{e}_1$. If $V=\R^2$, then $V=\R\vec{e}_1\oplus\R\vec{e}_2$.
    \item We know that $L^1(V)=V^*=R\vec{e}_1^*$. Thus, $\vec{e}_1^*\otimes\vec{e}_2^*:V\times V\to\R$. Thus, for example,
    \begin{equation*}
        (\vec{e}_1^*\otimes\vec{e}_2^*)((1,2),(3,4)) = \vec{e}_1^*(1,2)\cdot \vec{e}_2^*(3,4)
        = 1\cdot 4
        = 4
    \end{equation*}
    \item \marginnote{4/1:}Plan: More multilinear algebra.
    \begin{itemize}
        \item Properties of the tensor product.
        \item Sign of a permutation.
        \item Alternating tensors (lead into differential forms down the road).
    \end{itemize}
    \item Recall: $V$ is an $n$-dimensional vector space over $\R$ with basis $e_1,\dots,e_n$. $\lin[k]{V}$ is the vector space of $k$-tensors on $V$. $\{e_I^*\mid I\text{ a multiindex of }n\text{ of length }k\}$ is a basis for $\lin[k]{V}$.
    \item For example, if $V=\R^2$ and $T\in\lin[2]{V}$, then
    \begin{equation*}
        T(a_1e_1+a_2e_2,b_1e_1+b_2e_2) = a_1b_1T(e_1,e_1)+a_1b_2T(e_1,e_2)+a_2b_1T(e_2,e_1)+a_2b_2T(e_2,e_2)
    \end{equation*}
    \begin{itemize}
        \item A basis of $\lin[2]{V}$ is
        \begin{equation*}
            \{e_1^*\otimes e_1^*,e_1^*\otimes e_2^*,e_2^*\otimes e_1^*,e_2^*\otimes e_2^*\}
        \end{equation*}
        \item Recall that some basic properties are
        \begin{align*}
            e_1^*\otimes e_2^*((1,2),(3,4)) &= 1\cdot 4 = 4&
            e_2^*\otimes e_1^*((1,2),(3,4)) &= 2\cdot 3 = 6
        \end{align*}
        \item It follows by the initial decomposition of $T$ that
        \begin{equation*}
            T = a_1b_1e_1^*\otimes e_1^*+a_1b_2e_1^*\otimes e_2^*+a_2b_1e_2^*\otimes e_1^*+a_2b_2e_2^*\otimes e_2^*
        \end{equation*}
    \end{itemize}
    \item Important consequence: To know the action of $T$ on an arbitrary pair of vectors, you need only know its action on the basis; a higher-dimensional generalization of the earlier property.
    \item Note that
    \begin{equation*}
        e_I^*(e_J) = \delta_{IJ} =
        \begin{cases}
            1 & I=J\\
            0 & I\neq J
        \end{cases}
    \end{equation*}
    \item Basic properties of the tensor product.
    \begin{enumerate}
        \item \emph{Right-distributive}: If $T_1\in\lin[k]{V}$ and $T_2,T_3\in\lin[\ell]{V}$, then
        \begin{equation*}
            T_1\otimes(T_2+T_3) = T_1\otimes T_2+T_1\otimes T_3
        \end{equation*}
        \item \emph{Left-distributive}: If $T_1,T_2\in\lin[k]{V}$ and $T_3\in\lin[\ell]{V}$, then
        \begin{equation*}
            (T_1+T_2)\otimes T_3 = T_1\otimes T_3+T_2\otimes T_3
        \end{equation*}
        \item \emph{Associative}: If $T_1\in\lin[k]{V}$, $T_2\in\lin[\ell]{V}$, and $T_3\in\lin[m]{V}$, then
        \begin{equation*}
            T_1\otimes(T_2\otimes T_3) = (T_1\otimes T_2)\otimes T_2
            = T_1\otimes T_2\otimes T_3
        \end{equation*}
        \item \emph{Scalar multiplication}: If $T_1\in\lin[k]{V}$, $T_2\in\lin[\ell]{V}$, and $\lambda\in\R$, then
        \begin{equation*}
            (\lambda T_1)\otimes T_2 = \lambda(T_1\otimes T_2)
            = T_1\otimes(\lambda T_2)
        \end{equation*}
    \end{enumerate}
    \item Note that the tensor product is not commutative.
    \item Aside: Defining the sign of a permutation.
    \item $\bm{S_A}$: The set of all automorphisms of $A$ (bijections from $A$ to $A$), where $A$ is a set.
    \item $\bm{S_n}$: The set $S_{[n]}$.
    \item Given $\sigma_1,\sigma_2\in S_n$, $\sigma_1\circ\sigma_2\in S_n$.
    \begin{itemize}
        \item Thus, $S_n$ is a \textbf{group}.
    \end{itemize}
    \item \textbf{Transposition}: A function $\tau\in S_n$ such that
    \begin{equation*}
        \tau(k) =
        \begin{cases}
            j & k=i\\
            i & k=j\\
            k & k\neq i,j
        \end{cases}
    \end{equation*}
    for some $i,j\in[n]$. \emph{Denoted by} $\bm{\tau_{i,j}}$.
    \item Theorem: An element of $S_n$ can be written as the product of transpositions (i.e., for all $\sigma\in S_n$, there exist $\tau_1,\dots,\tau_m\in S_n$ such that $\sigma=\tau_1\circ\cdots\circ\tau_m$).
    \item \textbf{Sign} (of $\sigma\in S_n$): The number (mod 2) of transpositions whose product equals $\sigma$. \emph{Denoted by} $\bm{(-1)^\sigma}$, $\bm{\sgn(\sigma)}$.
    \item Theorem: The sign of $\sigma$ is well-defined. Additionally,
    \begin{equation*}
        (-1)^{\sigma_1\sigma_2} = (-1)^{\sigma_1}\cdot(-1)^{\sigma_2}
    \end{equation*}
    \item Example: Consider the identity permutation. $(-1)^\sigma=+1$. We can think of this as the product of zero transpositions or, for instance, as the product of the two transpositions $\tau_{1,2}\circ\tau_{1,2}$. Another example would be $\tau_{2,3}\circ\tau_{1,2}\circ\tau_{1,2}\circ\tau_{2,3}$.
    \item Theorem: Let $X_i$ be a rational or polynomial function for each $i\in\N$. Then
    \begin{equation*}
        (-1)^\sigma = \prod_{i<j}\frac{X_{\sigma(i)}-X_{\sigma(j)}}{X_i-X_j}
    \end{equation*}
    \item Example: For the permutation $\sigma=(1,2,3)$, we have
    \begin{align*}
        (-1)^\sigma &= \frac{X_{\sigma(1)}-X_{\sigma(2)}}{X_1-X_2}\cdot\frac{X_{\sigma(1)}-X_{\sigma(3)}}{X_1-X_3}\cdot\frac{X_{\sigma(2)}-X_{\sigma(3)}}{X_2-X_3}\\
        &= \frac{X_2-X_3}{X_1-X_2}\cdot\frac{X_2-X_1}{X_1-X_3}\cdot\frac{X_3-X_1}{X_2-X_3}\\
        &= \frac{-(X_1-X_2)}{X_1-X_2}\cdot\frac{-(X_1-X_3)}{X_1-X_3}\cdot\frac{X_2-X_3}{X_2-X_3}\\
        &= -1\cdot -1\cdot 1\\
        &= +1
    \end{align*}
    which squares with the fact that $\sigma=\tau_{1,2}\circ\tau_{2,3}$.
    \item Claims to verify with the above formula:
    \begin{enumerate}
        \item $\sgn(\sigma)\in\{\pm 1\}$.
        \item $\sgn(\tau_{i,j})=-1$.
        \item $\sgn(\sigma_1\sigma_2)=\sgn(\sigma_1)\sgn(\sigma_2)$.
    \end{enumerate}
    \item \marginnote{4/4:}Plan:
    \begin{itemize}
        \item More multilinear algebra.
        \item Alternating $k$-tensors --- 2 views:
        \begin{enumerate}
            \item As a subspace of $\lin[k]{V}$.
            \item As a quotient of $\lin[k]{V}$.
        \end{enumerate}
        \item Next time: Operators as alternating tensors.
        \begin{itemize}
            \item Wedge products.
            \item Interior products.
            \item Pullbacks.
        \end{itemize}
    \end{itemize}
    \item Recall: $\dim V=n$, $e_1,\dots,e_n$ a basis, $\lin[k]{V}$ the space of $k$-tensors, $\sigma\in S_k$ implies $(-1)^\sigma\in\{\pm 1\}$, key property: $(-1)^{\sigma_1\sigma_2}=(-1)^{\sigma_1}(-1)^{\sigma_2}$.
    \item $\bm{T^\sigma}$: The $k$-tensor over $V$ defined by
    \begin{equation*}
        T^\sigma(v_1,\dots,v_k) = T(v_{\bar{\sigma}(1)},\dots,v_{\bar{\sigma}(k)})
    \end{equation*}
    where $T\in\lin[k]{V}$, $\sigma\in S_k$, and $\bar{\sigma}$ denotes the inverse of $\sigma$.
    \item Example: $n=2$, $k=2$. Let $T=e_1^*\otimes e_2^*\in\lin[2]{V}$. Let $\sigma=\tau_{1,2}$. Then $T^\sigma=e_2^*\otimes e_1^*$.
    \item Another property is $e_I^\sigma=e_{\sigma(I)}^*$.
    \item Properties:
    \begin{enumerate}
        \item $T^{\sigma_1\sigma_2}=(T^{\sigma_1})^{\sigma_2}$.
        \item $(T_1+T_2)^\sigma=T_1^\sigma+T_2^\sigma$.
        \item $(cT)^\sigma=cT^\sigma$.
    \end{enumerate}
    \item Thus, you can view $\sigma:\lin[k]{V}\to\lin[k]{V}$ as a linear map!
    \item \textbf{Alternating $\bm{k}$-tensor}: A tensor $T\in\lin[k]{V}$ such that $T^\sigma=(-1)^\sigma T$ for all $\sigma\in S_k$.
    \begin{itemize}
        \item Equivalently, $T^\tau=-T$ for all $\tau\in S_k$.
    \end{itemize}
    \item An example of an alternating $2$-tensor when $\dim V=2$ is $T=e_1^*\otimes e_2^*-e_2^*\otimes e_1^*$.
    \begin{itemize}
        \item Naturally, $T^\tau_{1,2}=-T$, and $\tau_{1,2}$ is the unique transposition in $S_2$.
    \end{itemize}
    \item $e_1^*\otimes e_2^*$ is \emph{not} an alternating 2-tensor since $(e_1^*\otimes e_2^*)^\tau=e_2^*\otimes e_1^*\neq(-1)^\tau(e_1^*\otimes e_2^*)$.
    \item We can look at $n=2$, $k=1$ for ourselves.
    \item Note: If $T_1,T_2$ are both alternating $k$-tensors, then $T_1+T_2$ is also alternating, as is $cT_1$ for all $c\in\R$.
    \item $\bm{\alt[k]{V}}$: The vector space of alternating $k$-tensors.
    \item $\bm{\Alt(T)}$: The function $\Alt:\lin[k]{V}\to\lin[k]{V}$ defined by
    \begin{equation*}
        \Alt(T) = \sum_{\sigma\in S_k}(-1)^\sigma T^\sigma
    \end{equation*}
    \item Properties:
    \begin{enumerate}
        \item $\im(\Alt)=\alt[k]{V}$.
        \item $\lin[k]{V}/\ker(\Alt)=\lam[k]{V^*}$ is isomorphic to $\alt[k]{V}$.
        \item $\Alt(T)^\sigma=(-1)^\sigma\Alt(T)$.
        \begin{itemize}
            \item Proof:
            \begin{align*}
                \Alt(T)^{\sigma'} &= \left( \sum_{\sigma\in S_k}(-1)^\sigma T^\sigma \right)^{\sigma'}\\
                &= \sum_{\sigma\in S_k}(-1)^\sigma T^{\sigma\sigma'}\\
                &= (-1)^{\sigma'}\sum_{\sigma\in S_k}(-1)^{\sigma'}(-1)^\sigma T^{\sigma\sigma'}\\
                &= (-1)^{\sigma'}\sum_{\sigma\in S_k}(-1)^{\sigma\sigma'}T^{\sigma\sigma'}\\
                &= (-1)^{\sigma'}\Alt(T)
            \end{align*}
            \item The last equality holds because summing over all $\sigma$ is the same as summing over all $\sigma'\circ\sigma$.
            \item This implies $\im(\Alt)\leq\alt[k]{V}$.
        \end{itemize}
        \item If $T\in\alt[k]{T}$, $\Alt(T)=k!T$.
        \begin{itemize}
            \item We have
            \begin{align*}
                \Alt(T) &= \sum_{\sigma\in S_k}(-1)^\sigma T^\sigma\\
                &= \sum_{\sigma\in S_k}(-1)^\sigma(-1)^\sigma T\\
                &= \sum_{\sigma\in S_k}T\\
                &= k!T
            \end{align*}
            where $T^\sigma=(-1)^\sigma T$ since $T\in\alt[k]{V}$ by definition.
            \item This implies that $\im(\Alt)=\alt[k]{V}$: $\Alt(\frac{1}{k!}T)=T\in\alt[k]{V}$.
        \end{itemize}
        \item $\Alt(T^\sigma)=\Alt(T)^\sigma$.
        \item $\Alt:\lin[k]{V}\to\lin[k]{V}$ is linear.
    \end{enumerate}
    \item Warning: Some people take $\Alt(T)=\frac{1}{k!}\sum_{\sigma\in S_k}(-1)^\sigma T^\sigma$\footnote{Klug prefers this convention, but the text takes the other one.}.
    \item Example: $n=k=2$. We have
    \begin{equation*}
        \Alt(e_1^*\otimes e_2^*) = e_1^*\otimes e_2^*-e_2^*\otimes e_1^*
    \end{equation*}
    \item \textbf{Non-repeating} (multi-index $I$): A multi-index $I$ such that $i_{j_1}\neq i_{j_2}$ for all $j_1\neq j_2$.
    \item \textbf{Increasing} (multi-index $I$): A multi-index $I$ such that $i_1<\cdots<i_k$.
    \item Claim: $\{\Alt(e_I^*)\}$ where $I$ is non-repeating and increasing is a basis for $\alt[k]{V}$. There are $\binom{n}{k}$ of these; thus, $\dim\alt[k]{V}=\binom{n}{k}$.
    \item \marginnote{4/6:}Klug will be in Texas on Monday and thus is cancelling class on Monday. Homework is now due next Friday. We'll have weekly homeworks going forward after that.
    \item Plan:
    \begin{itemize}
        \item $\Alt:\lin[k]{V}\twoheadrightarrow\alt[k]{V}$\footnote{The two-headed right arrow denotes a surjective map.}.
        \item Goal: Identify $\ker(\Alt)=\ide[k]{V}$, where $\ide[k]{V}$ is the space of \textbf{redundant} $k$-tensors\footnote{The $\mathcal{I}$ in $\ide[k]{V}$ stands for "ideal."}.
        \item Then: Operations on alternating tensors, e.g.,
        \begin{itemize}
            \item Wedge product.
            \item Interior product.
            \item Orientations.
        \end{itemize}
    \end{itemize}
    \item Claim: $\{\Alt(e_I^*)\mid I\text{ non-repeating, increasing multi-index}\}$ is a basis for $\alt[k]{V}$.
    \begin{itemize}
        \item Left as an exercise to us.
    \end{itemize}
    \item \textbf{Redundant} ($k$-tensor): A $k$-tensor of the form
    \begin{equation*}
        \ell_1\otimes\cdots\otimes\ell_i\otimes\ell_i\otimes\ell_{i+2}\otimes\cdots\otimes\ell_k
    \end{equation*}
    where $\ell_1,\dots,\ell_k\in V^*$.
    \item $\bm{\ide[k]{V}}$: The span of all redundant $k$-tensors.
    \begin{itemize}
        \item Note that not every $k$-tensor in $\ide[k]{V}$ is a redundant.
    \end{itemize}
    \item \textbf{Decomposable} ($k$-tensor): A $k$-tensor of the form $\ell_1\otimes\cdots\otimes\ell_k$ for $\ell_i\in\lin[1]{V}$.
    \begin{itemize}
        \item It often suffices to prove things for decomposable tensors.
    \end{itemize}
    \item Properties.
    \begin{enumerate}
        \item If $T\in\ide[k]{V}$, then $\Alt(T)=0$, i.e., $\ide[k]{V}\leq\ker(\Alt)$.
        \begin{itemize}
            \item "Proof by example": If $T=\ell_1\otimes\ell_1\otimes\ell_2\otimes\ell_3$, then $T^{\tau_{1,2}}=T$. It follows from the properties of $\Alt$ that
            \begin{align*}
                \Alt(T) &= \Alt(T^{\tau_{1,2}})
                = (-1)^{\tau_{1,2}}\Alt(T)
                = -\Alt(T)\\
                2\Alt(T) &= 0\\
                \Alt(T) &= 0
            \end{align*}
        \end{itemize}
        \item If $T\in\ide[r]{V}$ and $T'\in\lin[s]{V}$, then
        \begin{equation*}
            T\otimes T'\in\ide[r+s]{V}
        \end{equation*}
        Similarly, if $T\in\lin[r]{V}$ and $T\in\ide[s]{V}$, then
        \begin{equation*}
            T\otimes T'\in\ide[r+s]{V}
        \end{equation*}
        \begin{itemize}
            \item Proof: It suffices to assume that $T$ is redundant. Obviously adding more tensors to the direct product will not change the redundancy of the initial tensor. Example: $\ell_1\otimes\ell_1\otimes\ell_2$ is just as redundant as $\ell_1\otimes\ell_1\otimes\ell_2\otimes T$.
        \end{itemize}
        \item If $T\in\lin[k]{V}$ and $\sigma\in S_k$, then
        \begin{equation*}
            T^\sigma = (-1)^\sigma T+S
        \end{equation*}
        for some $S\in\ide[k]{V}$.
        \begin{itemize}
            \item Proof by example: It suffices to check this for decomposable tensors (a tensor is just a sum of decomposable tensors). Take $k=2$. Let $T=\ell_1\otimes\ell_2$. Let $\sigma=\tau_{1,2}$. Then
            \begin{equation*}
                T^\sigma-(-1)^\sigma T = \ell_2\otimes\ell_1+\ell_1\otimes\ell_2
                = (\ell_1+\ell_2)\otimes(\ell_1+\ell_2)-\ell_1\otimes\ell_1-\ell_2\otimes\ell_2
            \end{equation*}
            \item Actual proof: It suffices to assume $T$ is decomposable. We induct on the number of transpositions needed to write $\sigma$ as a product of \textbf{adjacent} transpositions.
            \item Base case: $\sigma=\tau_{i,i+1}$. Then
            \begin{equation*}
                \begin{split}
                    T^{\tau_{i,i+1}}+T ={}& \ell_1\otimes\cdots\otimes(\ell_i+\ell_{i+1})\otimes(\ell_i+\ell_{i+1})\otimes\cdots\otimes\ell_k\\
                    &-\ell_1\otimes\cdots\otimes\ell_i\otimes\ell_i\otimes\cdots\otimes\ell_k\\
                    &-\ell_1\otimes\cdots\otimes\ell_{i+1}\otimes\ell_{i+1}\otimes\cdots\otimes\ell_k
                \end{split}
            \end{equation*}
            \item Inductive step: If $\sigma=\beta\tau$, then
            \begin{align*}
                T^\sigma &= T^{\beta\tau}\\
                &= (-1)^\tau T^\beta+\text{stuff in }\ide[k]{V}\\
                &= (-1)^\tau[(-1)^\beta T+\text{stuff in }\ide[k]{V}]+\text{stuff in }\ide[k]{V}
            \end{align*}
        \end{itemize}
        \item If $T\in\lin[k]{V}$, then
        \begin{equation*}
            \Alt(T) = k!T+W
        \end{equation*}
        for some $W\in\ide[k]{V}$.
        \begin{itemize}
            \item We have that
            \begin{align*}
                \Alt(T) &= \sum_{\sigma\in S_k}(-1)^\sigma T^\sigma\\
                &= \sum_{\sigma\in S_k}(-1)^\sigma[(-1)^\sigma T+S_\sigma]\\
                &= \sum_{\sigma\in S_k}T+\sum_{\sigma\in S_k}(-1)^\sigma S_\sigma\\
                &= k!T+W
            \end{align*}
        \end{itemize}
        \item $\ide[k]{V}=\ker(\Alt)$.
        \begin{itemize}
            \item We have that $\ide[k]{V}\leq\ker(\Alt)$ by property 1.
            \item Now suppose $T\in\ker(\Alt)$. Then $\Alt(T)=0$. Then by property 4,
            \begin{align*}
                \Alt(T) &= k!T+W\\
                0 &= k!T+W\\
                T &= -\frac{1}{k!}W \in \ide[k]{V}
            \end{align*}
        \end{itemize}
    \end{enumerate}
    \item Warning: If $T\in\alt[r]{V}$ and $T'\in\alt[s]{V}$, then we do not necessarily have $T\otimes T'\in\alt[r+s]{V}$.
    \begin{itemize}
        \item Example: $e_1^*,e_2^*\in\alt[1]{V}$ have $e_1^*\otimes e_2^*\notin\alt[2]{V}$.
    \end{itemize}
    \item \textbf{Adjacent} (transposition): A transposition of the form $\tau_{i,i+1}$.
    \item \marginnote{4/8:}Recall that $\alt[k]{V}\hookrightarrow\lin[k]{V}$\footnote{The hooked right arrow denotes an injective map.}
    \item Functoriality: $(A\circ B)^*=B^*\circ A^*$.
    \begin{itemize}
        \item $A^*$ takes $\lin[k]{W}\to\lin[k]{V}$ and $\alt[k]{W}\to\alt[k]{V}$.
    \end{itemize}
    \item $\dim(\lam[k]{V})=\binom{n}{k}$.
    \begin{itemize}
        \item Special case $k=n$: $\dim\lam[n]{V}=1$.
        \item If $A:V\to V$ induces a map $\lam[n]{V^*}\to\lam[n]{V^*}$ defined by the determinant.
    \end{itemize}
    \item Aside: $\lam[k]{V}$ is "exterior powers."
    \item Plan: Wedge products + basis for $\lam[k]{V}$.
    \item \textbf{Wedge product}: A function $\wedge:\lam[k]{V^*}\times\lam[\ell]{V^*}\to\lam[k+\ell]{V}$. 
    \begin{itemize}
        \item We denote elements of $\lam[k]{V^*}$ by $\omega_1,\omega_2$, etc.
    \end{itemize}
    \item If $\pi:\lin[k]{V}\to\lam[k]{V^*}$ sends $T\mapsto\omega$, $\omega_1=\pi(T_1)$, and $\omega_2=\pi(T_2)$, then $\omega_1\wedge\omega_2=\pi(T_1\otimes T_2)$.
    \begin{itemize}
        \item Note that $\ker(\pi)=\ide[k]{V}$.
    \end{itemize}
    \item Properties.
    \begin{enumerate}
        \item This is well defined, i.e., this does not depend on the choice of $T_1,T_2$.
        \begin{itemize}
            \item Consider $T_1+W_1,T_2+W_2$ with $W_1,W_2\in\ide[k]{V}$.
            \item We check that $\pi[(T_1+W_1)\otimes(T_2+W_2)]=\pi(T_1\otimes T_2)$.
            \item Since $W_1\otimes T_2,T_1\otimes W_2,W_1\otimes W_2\in\ide[k+\ell]{V}$, we have that
            \begin{align*}
                \pi[(T_1+W_1)\otimes(T_2+W_2)] &= \pi(T_1\otimes T_2+W_1\otimes T_2+T_1\otimes W_2+W_1\otimes W_2)\\
                &= \pi(T_1\otimes T_2)+\pi(W_1\otimes T_2)+\pi(T_1\otimes W_2)+\pi(W_1\otimes W_2)\\
                &= \pi(T_1\otimes T_2)
            \end{align*}
        \end{itemize}
        \item Associative: We have that
        \begin{equation*}
            \omega_1\wedge(\omega_2\wedge\omega_3)=(\omega_1\wedge\omega_2)\wedge\omega_3=\omega_1\wedge\omega_2\wedge\omega_3
        \end{equation*}
        \begin{itemize}
            \item Follows from the definition of $\wedge$ in terms of $\pi$ and properties of the tensor product.
        \end{itemize}
        \item Distributive: We have that
        \begin{align*}
            (\omega_1+\omega_2)\wedge\omega_3 &= \omega_1\wedge\omega_3+\omega_2\wedge\omega_3&
            \omega_1\wedge(\omega_2+\omega_3) &= \omega_1\wedge\omega_2+\omega_1\wedge\omega_3
        \end{align*}
        \begin{itemize}
            \item Follows from the definition of $\wedge$ in terms of $\pi$ and properties of the tensor product.
        \end{itemize}
        \item Linear: We have that
        \begin{equation*}
            (c\omega_1)\wedge\omega_2 = c(\omega_1\wedge\omega_2)
            = \omega_1\wedge(c\omega_2)
        \end{equation*}
        \begin{itemize}
            \item Follows from the definition of $\wedge$ in terms of $\pi$ and properties of the tensor product.
        \end{itemize}
        \item Anticommutative: We have that
        \begin{equation*}
            \omega_1\wedge\omega_2 = (-1)^{k\ell}\omega_2\wedge\omega_1
        \end{equation*}
        \begin{itemize}
            \item It suffices to assume that $w_1=\ell_1\wedge\cdots\wedge\ell_k,w_2=\ell_1'\wedge\cdots\wedge\ell_\ell'$.
            \begin{itemize}
                \item We have
                \begin{align*}
                    (\ell_1\wedge\cdots\wedge\ell_k)\wedge(\ell_1'\wedge\cdots\wedge\ell_\ell') = (-1)^k(\ell_1'\wedge\cdots\wedge\ell_\ell')\wedge(\ell_1\wedge\cdots\wedge\ell_k)
                \end{align*}
            \end{itemize}
            \item Let $\ell_1,\dots,\ell_k\in\lam[1]{V^*}=V^*=\lin[1]{V}$.
            \item Recall that $\ide[1]{V}=\{0\}$.
            \item Claim: $\ell_{\sigma(1)}\wedge\cdots\wedge\ell_{\sigma(k)}=(-1)^\sigma\ell_1\wedge\cdots\wedge\ell_k$ for all $\sigma\in S_k$.
            \begin{itemize}
                \item Recall that $T^\sigma=(-1)^\sigma T+W$ for some $W\in\ide[k]{V}$.
                \item Let $T=\ell_1\otimes\cdots\otimes\ell_k$.
                \item Then
                \begin{align*}
                    (\ell_1\otimes\cdots\otimes\ell_k)^\sigma &= \ell_{\sigma(1)}\otimes\cdots\otimes\ell_{\sigma(k)}\\
                    &= (-1)^\sigma\ell_1\otimes\cdots\otimes\ell_k+W
                \end{align*}
                \item Then hit both sides by $\pi$, noting that $\pi(W)=0$.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
    \item Example:
    \begin{enumerate}
        \item $n=2$, $k=\ell=1$. Consider $e_1^*,e_2^*\in\lin[1]{V}=V^*=\alt[1]{V}=\lam[1]{V^*}$. Then
        \begin{align*}
            e_1^*\wedge e_2^* &= (-1)e_2^*\wedge e_1^*&
            e_1^*\wedge e_1^* &= 0 = e_2^*\wedge e_2^*
        \end{align*}
        \item $n=4$. We have $e_1^*\wedge(3e_1^*+2e_2^*+3e_2^*)=3(e_1^*\wedge e_1^*)+2(e_1^*\wedge e_2^*)+3(e_1^*\wedge e_3^*)$. We also have $(e_1^*\wedge e_2^*)\wedge(e_1^*\wedge e_2^*)=0$.
    \end{enumerate}
    \item \marginnote{4/13:}Plan:
    \begin{itemize}
        \item Finish multilinear algebra.
        \item Basis for $\lam[k]{V^*}$.
        \item Talk a bit about pullbacks and the determinant.
        \item \textbf{Orientations} of vector spaces.
        \item The \textbf{interior product}.
    \end{itemize}
    \item Basis for $\lam[k]{V^*}$.
    \begin{itemize}
        \item Recall that $\{\Alt(e_I^*)\mid I\text{ is a nonrepeating, increasing partition of $n$ into $k$ parts}\}$ is a basis for $\alt[k]{V}$.
    \end{itemize}
    \item $\Alt$ is an isomorphism from $\lam[k]{V^*}$ to $\alt[k]{V}$.
    \item If we have an injective map from $\alt[k]{V}$ to $\lin[k]{V}$ and $\pi$ a projection map from $\lin[k]{V}$ to the quotient space $\alt[k]{V^*}$ gives rise to $\pi|_{\alt[k]{V}}$.
    \item Claim:
    \begin{enumerate}
        \item $\pi|_{\alt[k]{V}}$ is an isomorphism.
        \item $\pi(\Alt(e_I^*))=k!\pi(e_I^*)$.
    \end{enumerate}
    (2) implies that $\{\pi(e_I^*)=e_{i_1}^*\wedge\cdots\wedge e_{i_k}^*,\ I\text{ non-repeating and increasing}\}$ is a basis for $\lam[k]{V^*}$.
    \item Examples:
    \begin{enumerate}
        \item $n=2=\dim V$, $V=\R e_1\oplus\R e_2$.
        \begin{itemize}
            \item $\lam[0]{V^*}=\R$ since $\binom{n}{0}=1$.
            \item $\lam[1]{V^*}=\R e_1^*\oplus\R e_2^*$ since $\binom{n}{1}=2$.
            \item $\lam[2]{V^*}=\R e_1^*\wedge e_2^*$ since $\binom{n}{2}=1$.
            \begin{itemize}
                \item For the second to last one, note that $e_1^*\wedge e_2^*=-e_2^*\wedge e_1^*$.
            \end{itemize}
            \item $\lam[3]{V^*}=0$ since $\binom{2}{3}=0$.
            \begin{itemize}
                \item For the last one, note that all $e_1^*\wedge e_1^*\wedge e_2^*=0$.
            \end{itemize}
        \end{itemize}
        \item $n=3$, $V=\R e_1\oplus\R e_2\oplus\R e_3$.
        \begin{itemize}
            \item $\binom{n}{0}=1$: $\lam[0]{V^*}=\R$.
            \item $\binom{n}{1}=3$: $\lam[1]{V^*}=\R e_1^*\oplus\R e_2^*\oplus\R e_3^*$.
            \item $\binom{n}{2}=3$: $\lam[2]{V^*}=\R e_1^*\wedge e_2^*\oplus\R e_2^*\wedge e_3^*\oplus\R e_1^*\wedge e_3^*$.
            \item $\binom{n}{3}=1$: $\lam[3]{V^*}=\R e_1^*\wedge e_2^*\wedge e_3^*$.
            \item $\binom{n}{m}=0$ ($m>n$): $\lam[m]{V^*}=\lam[4]{V^*}=0$.
        \end{itemize}
    \end{enumerate}
    \item If $A:V\to W$, $\omega_1\in\lam[k]{W^*}$, $\omega_2\in\lam[\ell]{W^*}$, then
    \begin{equation*}
        A^*(\omega_1\wedge\omega_2) = A^*\omega_1\wedge A^*\omega_2
    \end{equation*}
    \item \textbf{Determinant}: Let $\dim V=n$. Let $A:V\to V$ be a linear transformation. This induces a pullback $A^*:\lam[n]{V^*}\to\lam[n]{V^*}$. The top exterior power $k=n$ implies $\binom{k}{n}=1$. We define $\det(A)$ to be the unique real number such that $A^*(v)=\det(A)v$.
    \item This determinant is the one we know.
    \begin{itemize}
        \item $A^*$ sends $e_1^*\wedge\cdots\wedge e_n^*$ to $A^*e_1^*\wedge\cdots\wedge A^*e_n^*$ which equals $A^*(e_1^*\wedge\cdots\wedge e_n^*)$ or $\det(A)$
    \end{itemize}
    \item Sanity check.
    \begin{enumerate}
        \item $\det(\id)=1$.
        \begin{itemize}
            \item $\id(e_1^*\wedge\cdots\wedge e_n^*)=\id e_1^*\wedge\cdots\wedge\id e_n^*=1\cdot e_1^*\wedge\cdots\wedge e_n^*$.
        \end{itemize}
        \item If $A$ is not an isomorphism, then $\det(A)=0$.
        \begin{itemize}
            \item If $A$ is not an isomorphism, then there exists $v_1\in\ker A$ with $v_1\neq 0$. Let $v_1^*,\dots,v_n^*$ be a basis of $V^*$. So the pullback of this wedge is the wedge of the pullbacks, but $A^*v_1^*=0$, so
            \begin{equation*}
                A^*(v_1^*\wedge\cdots\wedge v_n^*) = (A^*v_1^*)\wedge\cdots\wedge(A^*v_n^*)
                = 0\wedge\cdots\wedge(A^*v_n^*)
                = 0
                = 0\cdot v_1^*\wedge\cdots\wedge v_n^*
            \end{equation*}
        \end{itemize}
        \item $\det(AB)=\det(A)\det(B)$.
        \begin{itemize}
            \item Let $A:V\to V$ and $B:V\to V$.
            \item We have $(AB)^*=B^*A^*$; in particular, $n=k$, $V=W=U=V$.
        \end{itemize}
    \end{enumerate}
    \item Recall: If we pick a basis for $V$, $e_1,\dots,e_n$.
    \begin{itemize}
        \item Implies $[a_{ij}]=[A]_{e_1,\dots,e_n}^{e_1,\dots,e_n}$.
    \end{itemize}
    \item Does $\det(A)=\det([a_{ij}])=\sum_{\sigma\in S_n}(-1)^\sigma a_{1,\sigma(1)}\cdots a_{n,\sigma(n)}$?
    \begin{itemize}
        \item If $A:V\to V$, we know that $A^*:\lam[n]{V^*}\to\lam[n]{V^*}$ takes $e_1^*\wedge\cdots\wedge e_n^*\mapsto A^*(e_1^*\wedge\cdots\wedge e_n^*)$. We WTS
        \begin{equation*}
            A^*(e_1^*\wedge\cdots\wedge e_n^*) = \left[ \sum_{\sigma\in S_n}(-1)^\sigma a_{1,\sigma(1)}\cdots a_{n,\sigma(n)} \right]e_1^*\wedge\cdots\wedge e_n^*
        \end{equation*}
        \item We have that
        \begin{align*}
            A^*(e_1^*\wedge\cdots\wedge e_n^*) &= A^*e_1^*\wedge\cdots\wedge A^*e_n^*\\
            &= \left( \sum_{i_1=1}^na_{i_1,1}e_{i_1}^* \right)\wedge\cdots\wedge\left( \sum_{i_n=1}^na_{i_n,n}e_{i_n}^* \right)\\
            &= \sum_{i_1,\dots,i_n}a_{i_1,1}\cdots a_{i_n,n}e_{i_1}^*\wedge\cdots\wedge e_{i_n}^*\\
            &= \left[ \sum_{\sigma\in S_n}(-1)^\sigma a_{1,\sigma(1)}\cdots a_{n,\sigma(n)} \right]e_1^*\wedge\cdots\wedge e_n^*
        \end{align*}
        where the sign arises from the need to reorder $e_{i_1}^*\wedge\cdots\wedge e_{i_n}^*$ and the antisymmetry of the wedge product.
    \end{itemize}
    \item \marginnote{4/15:}Plan:
    \begin{itemize}
        \item Orientations.
        \item Interior product.
    \end{itemize}
    \item \textbf{Interior product}: We know that $\lam[k]{V^*}\cong\alt[k]{V}$. Fix $v\in V$. Define $\iota_v:\lam[k]{V^*}\to\lam[k-1]{V^*}$.
    \begin{itemize}
        \item Wrong way: We take $\iota_v:\lin[k]{V}\to\lin[k-1]{V}$.
        \begin{equation*}
            T \mapsto \sum_{r=1}^k(-1)^{r-1}T(v_1,\dots,v_r,\dots,v_{k-1})
        \end{equation*}
        \item Right way: First define $\varphi_v:\alt[k]{V}\to\alt[k-1]{V}$ by
        \begin{equation*}
            T \mapsto T_v(v_1,\dots,v_{k-1})=T(v,v_1,\dots,v_{k-1})
        \end{equation*}
        \begin{itemize}
            \item Check: $T_{v_1+v_2}=T_{v_1}+T_{v_2}$. $T_{\lambda v}=\lambda T_v$. $\varphi_v^{k-1}\circ\varphi_v^k=0$ implies $\varphi_v\circ\varphi_w=-\varphi_w\circ\varphi_v$.
        \end{itemize}
    \end{itemize}
    \item Properties:
    \begin{enumerate}
        \setcounter{enumi}{-1}
        \item $\iota_vT\in\lin[k-1]{V}$.
        \item $\iota_v$ is a linear map. This is all happening in the set $\Hom(\lin[k]{V},\lin[k-1]{V})$.
        \item $\iota_{v_1+v_2}=\iota_{v_1}+\iota_{v_2}$; $\iota_{\lambda v}=\lambda\iota_v$.
        \item "Product rule": If $T_1\in\lin[p]{V}$ and $T_1\in\lin[q]{V}$, then $\iota_v(T_1\otimes T_2)=\iota_vT_1\otimes T_2+(-1)^pT_1\otimes\iota_vT_2$.
        \item We have
        \begin{equation*}
            \iota_v(\ell_1\otimes\cdots\otimes\ell_k) = \sum_{r=1}^k(-1)^{r-1}\ell_r(v)\ell_1\otimes\cdots\otimes\hat{\ell}_r\otimes\cdots\otimes\ell_k
        \end{equation*}
        \item $\iota_v\circ\iota_v=0\in\Hom(\lin[k]{V},\lin[k-2]{V})$.
        \begin{itemize}
            \item Note that this is related to $d^2=0$ from the first day of class (alongside $\int_m\dd{w}=\int_{\partial m}w$).
            \item Proof: We induct on $k$. It suffices to prove the result for $T$ decomposable.
            \item Trivial base case for $k=1$.
            \item We have that
            \begin{align*}
                (\iota_v\circ\iota_v)(\ell_1\otimes\cdots\otimes\ell_{k-1}\otimes\ell) &= \iota_v(\iota_vT\otimes\ell+(-1)^{k-1}\ell(v)T)\\
                &= \iota_v(\iota_vT\otimes\ell)+(-1)^{k-1}\ell(v)\iota_vT\\
                &= (-1)^{k-2}\ell(v)\iota_vT+(-1)^{k-1}\ell(v)\iota_vT\\
                &= (-1)^{k-2}\ell(v)\iota_vT-(-1)^{k-2}\ell(v)\iota_vT\\
                &= 0
            \end{align*}
        \end{itemize}
        \item If $T\in\ide[k]{V}$, then $\iota_vT\in\ide[k-1]{V}$.
        \begin{itemize}
            \item Thus, $\iota_v$ induces a map $\iota_v:\lam[k]{V^*}\to\lam[k-1]{V^*}$.
            \item Proof: It suffices to check this for decomposables.
        \end{itemize}
        \item $\iota_{v_1}\circ\iota_{v_2}=-\iota_{v_2}\circ\iota_{v_1}$.
    \end{enumerate}
    \item Orientations:
    \begin{itemize}
        \item A vector space $V$ should have two orientations.
        \item Two bases $e_1,\dots,e_n$ and $f_1,\dots,f_n$ are \textbf{orientation equivalent} if $T:V\to V$ an isomorphism has positive determinant. Otherwise, they are \textbf{orientation-inequivalent}.
        \item An orientation on $V$ is a choice of equivalence classes of bases under the equivalence relation on bases.
        \item $T:V\to W$ given orientations, $T$ preserves or reverses orientations.
    \end{itemize}
    \item Fancy orientations.
    \begin{itemize}
        \item An orientation on a 1D vector space $L$ is a division into two halves.
        \item Def: An orientation of $V$ is an orientation of $\lam[n]{V^*}$.
    \end{itemize}
    \item We can prove that they're both the same.
    \begin{itemize}
        \item If $W$ and $V$ are both oriented, then $V/W$ gets a canonical orientation.
    \end{itemize}
\end{itemize}



\section{Chapter 1: Multilinear Algebra}
\emph{From \textcite{bib:DifferentialForms}.}
\begin{itemize}
    \item \marginnote{3/31:}\textcite{bib:DifferentialForms} defines real vector spaces, the operations on them, their basic properties, and the zero vector.
    \item \textbf{Linearly independent} (vectors $v_1,\dots,v_k$): A finite set of vectors $v_1,\dots,v_k\in V$ such that the map from $\R^k$ to $V$ defined by $(c_1,\dots,c_k)\mapsto c_1v_1+\cdots+c_kv_k$ is injective.
    \item \textbf{Spanning} (vectors $v_1,\dots,v_k$): We require that the above map is surjective.
    \item \textcite{bib:DifferentialForms} defines basis, finite-dimensional vector space, dimension, subspace, linear map, and kernel.
    \item \textbf{Image} (of $A:V\to W$): The range space of $A$, a subspace of $W$. \emph{Also known as} $\bm{\im(A)}$.
    \item \textcite{bib:DifferentialForms} defines the matrix of a linear map.
    \item \textbf{Inner product} (on $V$): A map $B:V\times V\to\R$ with the following three properties.
    \begin{itemize}
        \item \emph{Bilinearity}: For vectors $v,v_1,v_2,w\in V$ and $\lambda\in\R$, we have
        \begin{equation*}
            B(v_1+v_2,w) = B(v_1,w)+B(v_2,w)
        \end{equation*}
        and
        \begin{equation*}
            B(\lambda v,w) = \lambda B(v,w)
        \end{equation*}
        \item \emph{Symmetry}: For vectors $v,w\in V$, we have $B(v,w)=B(w,v)$.
        \item \emph{Positivity}: For every vector $v\in V$, we have $B(v,v)\geq 0$. Moreover, if $v\neq 0$, then $B(v,v)>0$.
    \end{itemize}
    \item \textbf{$\bm{W}$-coset}: A set of the form $\{v+w\mid w\in W\}$, where $W$ is a subspace $V$ and $v\in V$. \emph{Denoted by} $\bm{v+W}$.
    \begin{itemize}
        \item If $v_1-v_2\in W$, then $v_1+W=v_2+W$.
        \item It follows that the distinct $W$-cosets decompose $V$ into a disjoint collection of subsets of $V$.
    \end{itemize}
    \item \textbf{Quotient space} (of $V$ by $W$): The set of distinct $W$-cosets in $V$, along with the following definitions of vector addition and scalar multiplication.
    \begin{align*}
        (v_1+W)+(v_2+W) &= (v_1+v_2)+W&
        \lambda(v+W) &= (\lambda v)+W
    \end{align*}
    \emph{Denoted by} $\bm{V/W}$.
    \item \textbf{Quotient map}: The linear map $\pi:V\to V/W$ defined by
    \begin{equation*}
        \pi(v) = v+W
    \end{equation*}
    \begin{itemize}
        \item $\pi$ is surjective.
        \item Note that $\ker(\pi)=W$ since for all $w\in W$, $\pi(w)=w+W=0+W$, which is the zero vector in $V/W$.
    \end{itemize}
    \item If $V,W$ are finite dimensional, then
    \begin{equation*}
        \dim(V/W) = \dim(V)-\dim(W)
    \end{equation*}
    \item Proposition 1.2.9: Let $A:V\to U$ be a linear map. If $W\subset\ker(A)$, then there exists a unique linear map $A^\sharp:V/W\to U$ with the property that $A=A^\sharp\circ\pi$, where $\pi:V\to V/W$ is the quotient map.
    \begin{itemize}
        \item This proposition rephrases in terms of quotient spaces the fact that if $w\in W$, then $A(v+w)=Av$.
    \end{itemize}
    \item \textbf{Dual space} (of $V$): The set of all linear functions $\ell:V\to\R$, along with the following definitions of vector addition and scalar multiplication.
    \begin{align*}
        (\ell_1+\ell_2)(v) &= \ell_1(v)+\ell_2(v)&
        (\lambda\ell)(v) &= \lambda\cdot\ell(v)
    \end{align*}
    \emph{Denoted by} $V^*$.
    \item \textbf{Dual basis} (of $e_1,\dots,e_n$ a basis of $V$): The basis of $V^*$ consisting of the $n$ functions that take every $v=c_1e_1+\cdots+c_ne_n$ to one of the $c_i$. \emph{Denoted by} $\bm{e_1^*,...,e_n^*}$. \emph{Given by}
    \begin{equation*}
        e_i^*(v) = c_i
    \end{equation*}
    for all $v\in V$.
    \item Claim 1.2.12: If $V$ is an $n$-dimensional vector space with basis $e_1,\dots,e_n$, then $e_1^*,\dots,e_n^*$ is a basis of $V^*$.
    \begin{proof}
        We will first prove that $e_1^*,\dots,e_n^*$ spans $V^*$. Let $\ell\in V^*$ be arbitrary. Set $\lambda_i=\ell(e_i)$ for all $i\in[n]$. Define $\ell'=\sum_{i=1}^n\lambda_ie_i^*$. Then
        \begin{equation*}
            \ell'(e_j) = \sum_{i=1}^n\lambda_ie_i^*(e_j)
            = \lambda_j\cdot 1
            = \ell(e_j)
        \end{equation*}
        for all $j\in[n]$. Therefore, since $\ell,\ell'$ take identical values on the basis of $V$, $\ell=\ell'$, as desired.\par
        We now prove that $e_1^*,\dots,e_n^*$ is linearly independent. Let $\sum_{i=1}^n\lambda_ie_i^*=0$. Then for all $j\in[n]$,
        \begin{equation*}
            \lambda_j = \left( \sum_{i=1}^n\lambda_ie_i^* \right)(e_j)
            = 0
        \end{equation*}
        as desired.
    \end{proof}
    \item \textbf{Transpose} (of $A$): The map from $W^*$ to $V^*$ defined by $\ell\mapsto\ell\circ A$ for all $\ell\in W^*$. \emph{Denoted by} $\bm{A^*}$.
    \item Claim 1.2.15: If $e_1,\dots,e_n$ is a basis of $V$, $f_1,\dots,f_m$ is a basis of $W$, $e_1^*,\dots,e_n^*$ and $f_1^*,\dots,f_m^*$ are the corresponding dual bases, and $[a_{i,j}]$ is the $m\times n$ matrix of $A$ with respect to $\{e_j\},\{f_i\}$, then the linear map $A^*$ is defined in terms of $\{f_i^*\},\{e_j^*\}$ by the transpose matrix $(a_{j,i})$.
    \begin{proof}
        Let $[c_{j,i}]$ be the $n\times m$ matrix of $A^*$ with respect to $\{f_i^*\},\{e_j^*\}$. We seek to prove that $a_{i,j}=c_{j,i}$ ($1\leq i\leq m$, $1\leq j\leq n$).\par
        By the definition of $[a_{i,j}]$ and $[c_{j,i}]$, we have that
        \begin{align*}
            A^*f_i^* &= \sum_{k=1}^nc_{k,i}e_k^*&
            Ae_j &= \sum_{k=1}^ma_{k,j}f_k
        \end{align*}
        It follows that
        \begin{equation*}
            [A^*f_i^*](e_j) = \left[ \sum_{k=1}^nc_{k,i}e_k^* \right](e_j)
            = c_{j,i}
        \end{equation*}
        and
        \begin{equation*}
            [A^*f_i^*](e_j) = f_i^*(Ae_j)
            = f_i^*\left( \sum_{k=1}^ma_{k,j}f_k \right)
            = a_{i,j}
        \end{equation*}
        so transitivity implies the desired result.
    \end{proof}
    \item \marginnote{4/4:}$\bm{V^k}$: The set of all $k$-tuples $(v_1,\dots,v_k)$ where $v_1,\dots,v_k\in V$ a vector space.
    \begin{itemize}
        \item Note that
        \begin{equation*}
            V^k = \underbrace{V\oplus\cdots\oplus V}_{k\text{ times}}
        \end{equation*}
        where "$\oplus$" denotes the direct sum.
    \end{itemize}
    \item \textbf{Linear} (function in its $i^\text{th}$ variable): A function $T:V^k\to\R$ such that the map from $V$ to $\R$ defined by $v\mapsto T(v_1,\dots,v_{i-1},v,v_{i+1},\dots,v_k)$ is linear, where all $v_j$ save $v_i$ are fixed.
    \item \textbf{$\bm{k}$-linear} (function $T$): A function $T:V^k\to\R$ that is linear in its $i^\text{th}$ variable for $i=1,\dots,k$. \emph{Also known as} \textbf{$\bm{k}$-tensor}.
    \item $\bm{\lin[k]{V}}$: The set of all $k$-tensors in $V$.
    \begin{itemize}
        \item Since the sum $T_1+T_2$ of two $k$-linear functions $T_1,T_2:V^k\to\R$ is just another $k$-linear function, and $\lambda T_1$ is $k$-linear for all $\lambda\in\R$, we have that $\lin[k]{V}$ is a vector space.
    \end{itemize}
    \item Convention: 0-tensors are just the real numbers. Mathematically, we define
    \begin{equation*}
        \lin[0]{V} = \R
    \end{equation*}
    \item Note that $\lin[1]{V}=V^*$.
    \item Defines multi-indices of $n$ of length $k$.
    \item Lemma 1.3.5: If $n,k\in\N$, then there are exactly $n^k$ multi-indices of $n$ of length $k$.
    \item $\bm{T_I}$: The real number $T(e_{i_1},\dots,e_{i_k})$, where $T\in\lin[k]{V}$, $e_1,\dots,e_n$ is a basis of $V$, and $I$ is a multi-index of $n$ of length $k$.
    \item Proposition 1.3.7: The real numbers $T_I$ determine $T$, i.e., if $T,T'$ are $k$-tensors and $T_I=T_I'$ for all $I$, then $T=T'$.
    \begin{proof}
        We induct on $n$. For the base case $n=1$, $T\in(\R^k)^*$ and we have already proven this result. Now suppose inductively that the assertion is true for $n-1$. For each $e_i$, let $T_i$ be the $(k-1)$-tensor defined by
        \begin{equation*}
            (v_1,\dots,v_{n-1}) \mapsto T(v_1,\dots,v_{n-1},e_i)
        \end{equation*}
        Then for an arbitrary $v=c_1e_1+\cdots+c_ne_n$,
        \begin{equation*}
            T(v_1,\dots,v_{n-1},v) = \sum_{i=1}^nc_iT_i(v_1,\dots,v_{n-1})
        \end{equation*}
        so the $T_i$'s determine $T$. Applying the inductive hypothesis completes the proof.
    \end{proof}
    \item \textbf{Tensor product}: The function $\otimes:\lin[k]{V}\times\lin[\ell]{V}\to\lin[k+\ell]{V}$ defined by
    \begin{equation*}
        (T_1\otimes T_2)(v_1,\dots,v_{k+\ell}) = T_1(v_1,\dots,v_k)T_2(v_{k+1},\dots,v_{k+\ell})
    \end{equation*}
    for all $T_1\in\lin[k]{V}$ and $T_2\in\lin[\ell]{V}$.
    \item Note that by the definition of 0-tensors as real numbers, if $a\in\R$ and $T\in\lin[k]{V}$, then
    \begin{equation*}
        a\otimes T = T\otimes a = aT
    \end{equation*}
    \item Proposition 1.3.9: Associativity, distributivity of scalar multiplication, and left and right distributive laws for the tensor product.
    \item \textbf{Decomposable} ($k$-tensor): A $k$-tensor $T$ for which there exist $\ell_1,\dots,\ell_k\in V^*$ such that
    \begin{equation*}
        T = \ell_1\otimes\cdots\otimes\ell_k
    \end{equation*}
    \item Defines $e_I^*$.
    \item Theorem 1.3.13: $V$ a vector space with basis $e_1,\dots,e_n$ and $0\leq k\leq n$ implies the $k$-tensors $e_I^*$ form a basis of $\lin[k]{V}$.
    \begin{proof}
        Spanning: Let $T\in\lin[k]{V}$ be arbitrary. Define
        \begin{equation*}
            T' = \sum_IT_Ie_I^*
        \end{equation*}
        Since
        \begin{equation*}
            T_J' = T'(e_{j_1},\dots,e_{j_k})
            = \sum_IT_Ie_I^*(e_{j_1},\dots,e_{j_k})
            = T_Je_J^*(e_{j_1},\dots,e_{j_k})
            = T_J
        \end{equation*}
        for all $J$, Proposition 1.3.7 asserts that $T=T'$. Therefore, since every $T_I\in\R$, $T=T'\in\spn(e_I^*)$.\par
        Linear independence: Suppose
        \begin{equation*}
            T = \sum_Ic_Ie_I^* = 0
        \end{equation*}
        for some set of constants $c_I\in\R$. Then
        \begin{equation*}
            0 = T(e_{j_1},\dots,e_{j_k})
            = \sum_Ic_Ie_I^*(e_{j_1},\dots,e_{j_k})
            = c_J
        \end{equation*}
        for all $J$, as desired.
    \end{proof}
    \item Corollary 1.3.15: If $\dim V=n$, then $\dim(\lin[k]{V})=n^k$.
    \begin{proof}
        Follows immediately from Lemma 1.3.5.
    \end{proof}
    \item \textbf{Pullback} (of $T$ by the map $A$): The $k$-tensor $A^*T:V^k\to\R$ defined by
    \begin{equation*}
        (A^*T)(v_1,\dots,v_k) = T(Av_1,\dots,Av_k)
    \end{equation*}
    where $V,W$ are finite-dimensional vector spaces, $A:V\to W$ is linear, and $T\in\lin[k]{W}$.
    \item Proposition 1.3.18: The map $A^*:\lin[k]{W}\to\lin[k]{V}$ defined by $T\mapsto A^*T$ is linear.
    \item Identities:
    \begin{itemize}
        \item If $T_1\in\lin[k]{W}$ and $T_2\in\lin[m]{W}$, then
        \begin{equation*}
            A^*(T_1\otimes T_2) = A^*(T_1)\otimes A^*(T_2)
        \end{equation*}
        \item If $U$ is a vector space, $B:U\to V$ is linear, and $T\in\lin[k]{W}$, then $(AB)^*T=B^*(A^*T)$. Hence,
        \begin{equation*}
            (AB)^* = B^*A^*
        \end{equation*}
    \end{itemize}
    \item \marginnote{4/13:}$\bm{\Sigma_k}$: The set containing the natural numbers 1 through $k$. \emph{Given by}
    \begin{equation*}
        \Sigma_k = \{1,2,\dots,k\}
    \end{equation*}
    \item \textbf{Permutation of order $\bm{k}$}: A bijection on $\Sigma_k$. \emph{Denoted by} $\bm{\sigma}$.
    \item \textbf{Product} (of $\sigma_1,\sigma_2$): The composition $\sigma_1\circ\sigma_2$, i.e., the map
    \begin{equation*}
        i \mapsto \sigma_1(\sigma_2(i))
    \end{equation*}
    \emph{Denoted by} $\bm{\sigma_1\sigma_2}$.
    \item \textbf{Inverse} (of $\sigma$): The permutation of order $k$ which is the inverse bijection of $\sigma$. \emph{Denoted by} $\bm{\sigma^{-1}}$.
    \item \textbf{Permutation group} (of $\Sigma_k$): The set of all permutations of order $k$. \emph{Also known as} \textbf{symmetric group on $\bm{k}$ letters}. \emph{Denoted by} $\bm{S_k}$.
    \item Lemma 1.4.2: The group $S_k$ has $k!$ elements.
    \item \textbf{Transposition}: A permutation of order $k$ defined by
    \begin{equation*}
        \ell \mapsto
        \begin{cases}
            j & \ell=i\\
            i & \ell=j\\
            \ell & \ell\neq i,j
        \end{cases}
    \end{equation*}
    for all $\ell\in\Sigma_k$, where $i,j\in\Sigma_k$. \emph{Denoted by} $\bm{\tau_{i,j}}$.
    \item \textbf{Elementary transposition}: A transposition of the form $\pi_{i,i+1}$.
    \item Theorem 1.4.4: Every $\sigma\in S_k$ can be written as a product of (a finite number of) transpositions.
    \begin{proof}
        We induct on $k$.\par
        For the base case $k=2$, the identity permutation of $S_2$ is the "product" of zero transpositions, and the only other permutation is a transposition (the "product" of one transposition, namely itself).\par
        Now suppose inductively that we have proven the claim for $k-1$. Let $\sigma\in S_k$ be arbitrary. Suppose $\sigma(k)=i$. Then $\tau_{i,k}\sigma(k)=k$. Since $(\tau_{i,k}\sigma)|_{\Sigma_{k-1}}\in S_{k-1}$, we have by the inductive hypothesis that $(\tau_{i,k}\sigma)|_{\Sigma_{k-1}}=\tau_1\cdots\tau_m$ for some set of permutations $\tau_1,\dots,\tau_m\in S_{k-1}$. For each $\tau_j$ ($1\leq j\leq m$), define $\tau_j'\in S_k$
        \begin{equation*}
            \tau_j'(\ell) =
            \begin{cases}
                \tau_j(\ell) & \ell<k\\
                \ell & \ell=k
            \end{cases}
        \end{equation*}
        It follows that
        \begin{align*}
            \tau_{i,k}\sigma &= \tau_1'\cdots\tau_m'\\
            \sigma &= \tau_{i,k}\tau_1'\cdots\tau_m'
        \end{align*}
        as desired.
    \end{proof}
    \item Theorem 1.4.5: Every transposition can be written as a product of elementary transpositions.
    \begin{proof}
        Let $\tau_{i,j}\in S_k$, and let $i<j$ WLOG. Then we have that
        \begin{equation*}
            \tau_{i,j} = \prod_{\ell=i}^{i-1}\tau_{\ell,\ell+1}
        \end{equation*}
        as desired.
    \end{proof}
    \item Corollary 1.4.6: Every permutation can be written as a product of elementary transpositions.
    \item \textbf{Sign} (of $\sigma$): The number $\pm 1$ assigned to $\sigma$ by the expression
    \begin{equation*}
        \prod_{i<j}\frac{x_{\sigma(i)}-x_{\sigma(j)}}{x_i-x_j}
    \end{equation*}
    where $x_1,\dots,x_k$ are coordinate functions on $\R^k$. \emph{Denoted by} $\bm{(-1)^\sigma}$.
    \item Claim 1.4.9: The sign defines a group homomorphism $S_k\to\{\pm 1\}$. That is, for $\sigma_1,\sigma_2\in S_k$, we have
    \begin{equation*}
        (-1)^{\sigma_1\sigma_2} = (-1)^{\sigma_1}(-1)^{\sigma_2}
    \end{equation*}
    \begin{proof}
        For all $i<j$, define $p,q$ such that $p$ is the lesser of $\sigma_2(i),\sigma_2(j)$ and $q$ is the greater of $\sigma_2(i),\sigma_2(j)$. Formally,
        \begin{align*}
            p &=
            \begin{cases}
                \sigma_2(i) & \sigma_2(i)<\sigma_2(j)\\
                \sigma_2(j) & \sigma_2(j)<\sigma_2(i)
            \end{cases}&
            q &=
            \begin{cases}
                \sigma_2(j) & \sigma_2(i)<\sigma_2(j)\\
                \sigma_2(i) & \sigma_2(j)<\sigma_2(i)
            \end{cases}
        \end{align*}
        It follows that if $\sigma_2(i)<\sigma_2(j)$, then
        \begin{equation*}
            \frac{x_{\sigma_1\sigma_2(i)}-x_{\sigma_1\sigma_2(j)}}{x_{\sigma_2(i)}-x_{\sigma_2(j)}} = \frac{x_{\sigma_1(p)}-x_{\sigma_1(q)}}{x_p-x_q}
        \end{equation*}
        and if $\sigma_2(j)<\sigma_2(i)$, then
        \begin{equation*}
            \frac{x_{\sigma_1\sigma_2(i)}-x_{\sigma_1\sigma_2(j)}}{x_{\sigma_2(i)}-x_{\sigma_2(j)}} = \frac{x_{\sigma_1(q)}-x_{\sigma_1(p)}}{x_q-x_p}
            = \frac{x_{\sigma_1(p)}-x_{\sigma_1(q)}}{x_p-x_q}
        \end{equation*}
        Therefore,
        \begin{align*}
            (-1)^{\sigma_1\sigma_2} &= \prod_{i<j}\frac{x_{\sigma_1\sigma_2(i)}-x_{\sigma_1\sigma_2(j)}}{x_i-x_j}\\
            &= \prod_{i<j}\frac{x_{\sigma_1\sigma_2(i)}-x_{\sigma_1\sigma_2(j)}}{x_{\sigma_2(i)}-x_{\sigma_2(j)}}\cdot\frac{x_{\sigma_2(i)}-x_{\sigma_2(j)}}{x_i-x_j}\\
            &= \prod_{i<j}\frac{x_{\sigma_1(p)}-x_{\sigma_1(q)}}{x_p-x_q}\cdot\prod_{i<j}\frac{x_{\sigma_2(i)}-x_{\sigma_2(j)}}{x_i-x_j}\\
            &= (-1)^{\sigma_1}(-1)^{\sigma_2}
        \end{align*}
        as desired.
    \end{proof}
    \item Proposition 1.4.11: If $\sigma$ is the product of an odd number of transpositions, then $(-1)^\sigma=-1$, and if $\sigma$ is the product of an even number of transpositions, then $(-1)^\sigma=+1$.
    \begin{proof}
        Follows from the fact that $(-1)^\sigma=-1$ (see Exercise 1.4.ii).
    \end{proof}
    \item $\bm{T^\sigma}$: The $k$-tensor defined by
    \begin{equation*}
        T^\sigma(v_1,\dots,v_k) = T(v_{\sigma^{-1}(1)},\dots,v_{\sigma^{-1}(k)})
    \end{equation*}
    where $T\in\lin[k]{V}$, $V$ is an $n$-dimensional vector space, and $\sigma\in S_k$.
    \item Proposition 1.4.14:
    \begin{enumerate}
        \item If $T=\ell_1\otimes\cdots\otimes\ell_k$ ($\ell_i\in V^*$), then $T^\sigma=\ell_{\sigma(1)}\otimes\cdots\otimes\ell_{\sigma(k)}$.
        \begin{proof}
            If $v_1,\dots,v_k\in V$, then
            \begin{align*}
                T^\sigma(v_1,\dots,v_k) &= T(v_{\sigma^{-1}(1)},\dots,v_{\sigma^{-1}(k)})\\
                &= [\ell_1\otimes\cdots\otimes\ell_k](v_{\sigma^{-1}(1)},\dots,v_{\sigma^{-1}(k)})\\
                &= \ell_1(v_{\sigma^{-1}(1)})\cdots\ell_k(v_{\sigma^{-1}(k)})\\
                &= \ell_{\sigma(1)}(v_1)\cdots\ell_{\sigma(k)}(v_2)\\
                &= [\ell_{\sigma(1)}\otimes\cdots\otimes\ell_{\sigma(k)}](v_1,\dots,v_k)
            \end{align*}
            as desired. Note that we can justify the fourth equality by nothing that if $\sigma^{-1}(i)=q$, then the $i^\text{th}$ term in the product is $\ell_{\sigma(q)}(v_q)$, so since $\sigma$ is a bijection, the product can be arranged to the form on the right-hand side of equality four.
        \end{proof}
        \item The assignment $T\mapsto T^\sigma$ is a linear map from $\lin[k]{V}\to\lin[k]{V}$.
        \begin{proof}
            See Exercise 1.4.iii.
        \end{proof}
        \item If $\sigma_1,\sigma_2\in S_k$, we have $T^{\sigma_1\sigma_2}=(T^{\sigma_1})^{\sigma_2}$.
        \begin{proof}
            Let $T=\ell_1\otimes\cdots\otimes\ell_k$\footnote{What gives us the right to assume $T$ is decomposable?}. Then
            \begin{equation*}
                T^{\sigma_1} = \ell_{\sigma_1(1)}\otimes\cdots\otimes\ell_{\sigma_1(k)}
                = \ell_1'\otimes\cdots\otimes\ell_k'
            \end{equation*}
            and thus
            \begin{equation*}
                (T^{\sigma_1})^{\sigma_2} = \ell_{\sigma_2(1)}'\otimes\cdots\otimes\ell_{\sigma_2(k)}'
            \end{equation*}
            Let $\sigma_2(i)=j$. Then since $\ell_p'=\ell_{\sigma_1(p)}$ by definition, we have that $\ell_{\sigma_2(j)}'=\ell_{\sigma_1(\sigma_2(j))}$. Therefore,
            \begin{align*}
                (T^{\sigma_1})^{\sigma_2} &= \ell_{\sigma_2(1)}'\otimes\cdots\otimes\ell_{\sigma_2(k)}'\\
                &= \ell_{\sigma_1(\sigma_2(1))}\otimes\cdots\otimes\ell_{\sigma_1(\sigma_2(k))}\\
                &= \ell_{\sigma_1\sigma_2(1)}\otimes\cdots\otimes\ell_{\sigma_1\sigma_2(k)}\\
                &= T^{\sigma_1\sigma_2}
            \end{align*}
            as desired.
        \end{proof}
    \end{enumerate}
    \item \textbf{Alternating} ($k$-tensor): A $k$-tensor $T\in\lin[k]{V}$ such that $T^\sigma=(-1)^\sigma T$ for all $\sigma\in S_k$.
    \item $\bm{\alt[k]{V}}$: The set of all alternating $k$-tensors in $\lin[k]{V}$.
    \begin{itemize}
        \item Proposition 1.4.14(2) implies that $(T_1+T_2)^\sigma=T_1^\sigma+T_2^\sigma$ and $(\lambda T)^\sigma=\lambda T^\sigma$; it follows that $\alt[k]{V}$ is a vector space.
    \end{itemize}
    \item \textbf{Alternation operation}: The function from $\lin[k]{V}\to\lin[k]{V}$ defined by
    \begin{equation*}
        T \mapsto \sum_{\tau\in S_k}(-1)^\tau T^\tau
    \end{equation*}
    \emph{Denoted by} $\bm{\Alt{}}$.
    \item Proposition 1.4.17: For $T\in\lin[k]{V}$ and $\sigma\in S_k$, we have that
    \begin{enumerate}
        \item $\Alt(T)^\sigma=(-1)^\sigma\Alt T$.
        \begin{proof}
            We have that
            \begin{align*}
                \Alt(T)^\sigma &= \left( \sum_{\tau\in S_k}(-1)^\tau T^\tau \right)^\sigma\\
                &= \sum_{\tau\in S_k}(-1)^\tau(T^\tau)^\sigma\tag*{Proposition 1.4.14(2)}\\
                &= \sum_{\tau\in S_k}(-1)^\tau T^{\tau\sigma}\tag*{Proposition 1.4.14(3)}\\
                &= (-1)^\sigma\sum_{\tau\in S_k}(-1)^{\tau\sigma}T^{\tau\sigma}\\
                &= (-1)^\sigma\sum_{\tau\sigma\in S_k}(-1)^{\tau\sigma}T^{\tau\sigma}\\
                &= (-1)^\sigma\Alt T
            \end{align*}
            as desired.
        \end{proof}
        \item If $T\in\alt[k]{V}$, then $\Alt T=k!T$.
        \begin{proof}
            Since $T\in\alt[k]{V}$, we know that $T^\sigma=(-1)^\sigma T$. Therefore,
            \begin{equation*}
                \Alt T = \sum_{\tau\in S_k}(-1)^\tau T^\tau
                = \sum_{\tau\in S_k}(-1)^\tau(-1)^\tau T
                = \sum_{\tau\in S_k}T
                = k!T
            \end{equation*}
            where the last equality holds because the cardinality of $S_k$ is $k!$.
        \end{proof}
        \item $\Alt(T^\sigma)=\Alt(T)^\sigma$.
        \begin{proof}
            We have that
            \begin{equation*}
                \Alt(T^\sigma) = \sum_{\tau\in S_k}(-1)^\tau T^{\tau\sigma}
                = (-1)^\sigma\sum_{\tau\in S_k}(-1)^{\tau\sigma}T^{\tau\sigma}
                = (-1)^\sigma\Alt(T)
                = \Alt(T)^\sigma
            \end{equation*}
            as desired.
        \end{proof}
        \item The alternation operation is linear.
        \begin{proof}
            Follows by Proposition 1.4.14.
        \end{proof}
    \end{enumerate}
    \item \textbf{Repeating} (multi-index $I$): A multi-index $I$ of length $k$ such that $i_r=i_s$ for some $r\neq s$.
    \item \textbf{Strictly increasing} (multi-index $I$): A multi-index $I$ of length $k$ such that $i_1<i_2<\cdots<i_r$.
    \item $\bm{I^\sigma}$: The multi-index of length $k$ defined by
    \begin{equation*}
        I^\sigma = (i_{\sigma(1)},\dots,i_{\sigma(k)})
    \end{equation*}
    \item If $I$ is non-repeating, there is a unique $\sigma\in S_k$ such that $I^\sigma$ is strictly increasing.
    \item $\bm{\psi_I}$: The following $k$-tensor. \emph{Given by}
    \begin{equation*}
        \psi_I = \Alt(e_I^*)
    \end{equation*}
    \item Proposition 1.4.20:
    \begin{enumerate}
        \item $\psi_{I^\sigma}=(-1)^\sigma\psi_I$.
        \begin{proof}
            We have that
            \begin{equation*}
                \psi_{I^\sigma} = \Alt(e_{I^\sigma}^*)
                = \Alt[(e_I^*)^\sigma]
                = \Alt(e_I^*)^\sigma
                = (-1)^\sigma\Alt(e_I^*)
                = (-1)^\sigma\psi_I
            \end{equation*}
            as desired.
        \end{proof}
        \item If $I$ is repeating, then $\psi_I=0$.
        \begin{proof}
            Suppose $I=(i_1,\dots,i_k)$ is such that $i_r=i_s$ for some distinct $r,s\in\Sigma_k$. Then $e_I^*=e_{I^{\tau_{i_r,i_s}}}^*$, so
            \begin{equation*}
                \psi_I = \psi_{I^{\tau_{i_r,i_s}}}
                = (-1)^{\tau_{i_r,i_s}}\psi_I
                = -\psi_I
            \end{equation*}
            Therefore, we must have $\psi_I=0$, as desired.
        \end{proof}
        \item If $I$ and $J$ are strictly increasing, then
        \begin{equation*}
            \psi_I(e_{j_1},\dots,e_{j_k}) =
            \begin{cases}
                1 & I=J\\
                0 & I\neq J
            \end{cases}
        \end{equation*}
        \begin{proof}
            We have by definition that
            \begin{equation*}
                \psi_I(e_{j_1},\dots,e_{j_k}) = \sum_\tau(-1)^\tau e_{I^\tau}^*(e_{j_1},\dots,e_{j_k})
            \end{equation*}
            This combined with the facts that
            \begin{equation*}
                e_{I^\tau}^*(e_{j_1},\dots,e_{j_k}) =
                \begin{cases}
                    1 & I^\tau=J\\
                    0 & I^\tau\neq J
                \end{cases}
            \end{equation*}
            $I^\tau$ is strictly increasing iff $I^\tau=I$, and the above equation is nonzero iff $I^\tau=I=J$ implies the desired result.
        \end{proof}
    \end{enumerate}
    \item Conclusion 1.4.22: If $T\in\alt[k]{V}$, then we can write $T$ as a sum
    \begin{equation*}
        T = \sum_Ic_I\psi_I
    \end{equation*}
    with $I$'s strictly increasing.
    \begin{proof}
        Let $T\in\alt[k]{V}$ be arbitrary. By Theorem 1.3.13,
        \begin{equation*}
            T = \sum_Ja_Je_J^*
        \end{equation*}
        for some set of $a_J\in\R$. It follows since $\Alt(T)=k!T$ that
        \begin{equation*}
            T = \frac{1}{k!}\sum a_J\Alt(e_J^*)
            = \sum b_J\psi_J
        \end{equation*}
        We can disregard all repeating terms in the sum since they are zero by Proposition 1.4.20(2); for every non-repeating term $J$, we can write $J=I^\sigma$, where $I$ is strictly increasing and hence $\psi_J=(-1)^\sigma\psi_I$.
    \end{proof}
    \item Claim 1.4.24: The $c_I$'s of Conclusion 1.4.22 are unique.
    \begin{proof}
        For $J$ strictly increasing, we have
        \begin{equation*}
            T_J = T(e_{j_1},\dots,e_{j_k})
            = \sum_Ic_I\psi_I(e_{j_1},\dots,e_{j_k})
            = c_J
        \end{equation*}
    \end{proof}
    \item Proposition 1.4.26: The alternating tensors $\psi_I$ with $I$ strictly increasing are a basis for $\alt[k]{V}$.
    \begin{proof}
        Spanning: See Conclusion 1.4.22.\par
        Linear independence: See Claim 1.4.24.
    \end{proof}
    \item We have that
    \begin{equation*}
        \dim\alt[k]{V} = \binom{n}{k}
        = \frac{n!}{(n-k)!k!}
    \end{equation*}
    \begin{itemize}
        \item Hint in proving this claim: "Show that every strictly increasing multi-index of length $k$ determines a $k$-element subset of $\{1,\dots,n\}$ and vice versa." \parencite[16]{bib:DifferentialForms}.
        \item Note also that if $k>n$, every multi-index has a repeat somewhere, meaning that $\dim\alt[k]{V}=\binom{n}{k}=0$.
    \end{itemize}
    \item \marginnote{4/14:}Having discussed $\im(\Alt)=\alt[k]{V}$ in some detail now, we move onto $\ker(\Alt)$.
    \item \textbf{Redundant} (decomposable $k$-tensor): A decomposable $k$-tensor $\ell_1\otimes\cdots\otimes\ell_k$ such that for some $i\in[k-1]$, $\ell_i=\ell_{i+1}$.
    \item $\bm{\ide[k]{V}}$: The linear span of the set of redundant $k$-tensors.
    \item Convention: There are no redundant 1-tensors. Mathematically, we define
    \begin{equation*}
        \ide[1]{V} = 0
    \end{equation*}
    \item Proposition 1.5.2: $T\in\ide[k]{V}$ implies $\Alt(T)=0$.
    \begin{proof}
        Let $T=\ell_1\otimes\cdots\otimes\ell_k$ with $\ell_i=\ell_{i+1}$. Then if $\sigma=\tau_{i,i+1}$, we have that $T^\sigma=T$ and $(-1)^\sigma=-1$. Therefore,
        \begin{align*}
            \Alt(T) &= \Alt(T^\sigma)\\
            &= \Alt(T)^\sigma\tag*{Proposition 1.4.17(3)}\\
            &= (-1)^\sigma\Alt(T)\tag*{Proposition 1.4.17(1)}\\
            &= -\Alt(T)
        \end{align*}
        so we must have that $\Alt(T)=0$, as desired.
    \end{proof}
    \item Proposition 1.5.3: $T\in\ide[r]{V}$ and $T'\in\lin[s]{V}$ imply $T\otimes T',T'\otimes T\in\ide[r+s]{V}$.
    \begin{proof}
        We first justify why we need only prove this claim for $T'$ decomposable. As an element of $\lin[s]{V}$, we know that $T'=\sum a_Ie_I^*$ for some set of $a_I\in\R$. Since each $e_I^*$ is decomposable, this means that $T'$ is a linear combination of decomposable tensors. This combined with the fact that the tensor product is linear means that
        \begin{equation*}
            T\otimes T' = T\otimes\sum a_Ie_I^*
            = \sum a_I(T\otimes e_I^*)
        \end{equation*}
        and similarly for $T'\otimes T$. Thus, if we can prove that each $T\otimes e_I^*\in\ide[r+s]{V}$, it will follow since $\ide[k]{V}$ is a vector space that $\sum a_I(T\otimes e_I^*)=T\otimes T'\in\ide[r+s]{V}$. In other words, we need only prove that $T\otimes T'\in\ide[r+s]{V}$ for $T'$ decomposable, as desired.\par
        Let $T=\ell_1\otimes\cdots\otimes\ell_r$ with $\ell_i=\ell_{i+1}$, and let $T'=\ell_1'\otimes\cdots\otimes\ell_s'$. It follows that
        \begin{equation*}
            T\otimes T' = (\ell_1\otimes\cdots\otimes\ell_i\otimes\ell_{i+1}\otimes\cdots\otimes\ell_r)\otimes(\ell_1'\otimes\cdots\otimes\ell_s')
        \end{equation*}
        is redundant and hence in $\ide[r+s]{V}$, as desired. The argument is symmetric for $T'\otimes T$.
    \end{proof}
    \item Proposition 1.5.4: $T\in\lin[k]{V}$ and $\sigma\in S_k$ imply
    \begin{equation*}
        T^\sigma = (-1)^\sigma T+S
    \end{equation*}
    where $S\in\ide[k]{V}$.
    \begin{proof}
        As with Proposition 1.5.3, the linearity of $\sigma:\lin[k]{V}\to\lin[k]{V}$ allows us to assume that $T$ is decomposable.\par
        By Theorem 1.4.5, $\sigma$ can be written as a product of $m$ elementary transpositions. To prove the claim, we induct on $m$.\par
        For the base case $m=1$, let $\sigma=\tau_{i,i+1}$. If $T_1=\ell_1\otimes\cdots\otimes\ell_{i-1}$ and $T_2=\ell_{i+2}\otimes\cdots\otimes\ell_k$, then
        \begin{align*}
            T^\sigma-(-1)^\sigma T &= T_1\otimes(\ell_{i+1}\otimes\ell_i\pm\ell_i\otimes\ell_{i+1})\otimes T_2\\
            &= T_1\otimes[(\ell_i+\ell_{i+1})\otimes(\ell_i+\ell_{i+1})\mp\ell_i\otimes\ell_i\mp\ell_{i+1}\otimes\ell_{i+1}]\otimes T_2
        \end{align*}
        i.e., $T^\sigma-(-1)^\sigma T$ is the sum of three redundant $k$-tensors, and thus a redundant $k$-tensor in and of itself, as desired. Note that even though only the middle portion is explicitly redundant, Proposition 1.5.3 allows us to call the whole tensor product redundant.\par
        Now suppose inductively that we have proven the claim for $m-1$. Let $\sigma=\tau\beta$ where $\beta$ is the product of $m-1$ elementary transpositions and $\tau$ is an elementary transposition. Then
        \begin{align*}
            T^\sigma &= (T^\beta)^\tau\tag*{Proposition 1.4.14(3)}\\
            &= (-1)^\tau T^\beta+\cdots\tag*{Base case}\\
            &= (-1)^\tau(-1)^\beta T+\cdots\tag*{Inductive hypothesis}\\
            &= (-1)^\sigma T+\cdots\tag*{Claim 1.4.9}
        \end{align*}
        where the dots are elements of $\ide[k]{V}$.
    \end{proof}
    \item Corollary 1.5.6: $T\in\lin[k]{V}$ implies
    \begin{equation*}
        \Alt(T) = k!T+W
    \end{equation*}
    where $W\in\ide[k]{V}$.
    \begin{proof}
        By definition,
        \begin{equation*}
            \Alt(T) = \sum_{\sigma\in S_k}(-1)^\sigma T^\sigma
        \end{equation*}
        By Proposition 1.5.4,
        \begin{equation*}
            T^\sigma = (-1)^\sigma T+W_\sigma
        \end{equation*}
        for all $\sigma\in S_k$ with each $W_\sigma\in\ide[k]{V}$. It follows by combining the above two results that
        \begin{equation*}
            \Alt(T) = \sum_{\sigma\in S_k}(-1)^\sigma[(-1)^\sigma T+W_\sigma]
            = \sum_{\sigma\in S_k}T+\sum_{\sigma\in S_k}(-1)^\sigma W_\sigma
            = k!T+W
        \end{equation*}
        where $W=\sum_{\sigma\in S_k}(-1)^\sigma W_\sigma$ is an element of $\ide[k]{V}$ as a linear combination of elements of $\ide[k]{V}$.
    \end{proof}
    \item Corollary 1.5.8: Let $V$ be a vector space and $k\geq 1$. Then
    \begin{equation*}
        \ide[k]{V} = \ker(\Alt:\lin[k]{V}\to\alt[k]{V})
    \end{equation*}
    \begin{proof}
        Suppose first that $T\in\ide[k]{V}$. Then by Proposition 1.5.2, $\Alt(T)=0$, so $T\in\ker(\Alt)$, as desired.\par
        Now suppose that $T\in\ker(\Alt)$. Then $\Alt(T)=0$, so by Corollary 1.5.6,
        \begin{align*}
            0 &= k!T+W\\
            T &= -\frac{1}{k!}W
        \end{align*}
        Therefore, as a scalar multiple of an element of $\ide[k]{V}$, $T\in\ide[k]{V}$.
    \end{proof}
    \item Theorem 1.5.9: Every $T\in\lin[k]{V}$ has a unique decomposition $T=T_1+T_2$ where $T_1\in\alt[k]{V}$ and $T_2\in\ide[k]{V}$.
    \begin{proof}
        By Corollary 1.5.6, we have that
        \begin{align*}
            \Alt(T) &= k!T+W\\
            T &= \underbrace{\left( \frac{1}{k!}\Alt(T) \right)}_{T_1}+\underbrace{\left( -\frac{1}{k!}W \right)}_{T_2}
        \end{align*}
        As to uniqueness, suppose $0=T_1+T_2$ where $T_1\in\alt[k]{V}$ and $T_2\in\ide[k]{V}$. Then
        \begin{align*}
            0 &= \Alt(0)
                = \Alt(T_1+T_2)
                = \Alt(T_1)+\Alt(T_2)
                = k!T_1+0
                = k!T_1\\
            T_1 &= 0
        \end{align*}
        so $T_2=0$, too.
    \end{proof}
    \item $\bm{\lam[k]{V^*}}$: The quotient of the vector space $\lin[k]{V}$ by the subspace $\ide[k]{V}$. \emph{Given by}
    \begin{equation*}
        \lam[k]{V^*} = \lin[k]{V}/\ide[k]{V}
    \end{equation*}
    \item The quotient map $\pi:\lin[k]{V}\to\lam[k]{V^*}$ defined by $T\mapsto T+\ide[k]{V}$ is onto and has $\ker(\pi)=\ide[k]{V}$.
    \item Theorem 1.5.13: $\pi:\lin[k]{V}\to\lam[k]{V^*}$ maps $\alt[k]{V}$ bijectively onto $\lam[k]{V^*}$.
    \begin{proof}
        Theorem 1.5.9 implies that every $T+\ide[k]{V}$ contains a unique $T_1\in\alt[k]{V}$. Thus, for every element of $\lam[k]{V^*}$, there is a unique element of $\alt[k]{V}$ which gets mapped onto it by $\pi$.
    \end{proof}
    \item Note that since $\alt[k]{V}$ and $\lam[k]{V}$ are in bijective correspondence, many texts do not distinguish between them. There are some advantages to making the distinction, though.
    \item The tensor product and pullback operatios give rise to similar operations on the spaces $\lam[k]{V^*}$.
    \item \textbf{Wedge product}: The function $\wedge:\lam[k_1]{V^*}\times\lam[k_2]{V^*}\to\lam[k_1+k_2]{V^*}$ defined by
    \begin{equation*}
        \omega_1\wedge\omega_2 = \pi(T_1\otimes T_2)
    \end{equation*}
    where for $i=1,2$, $\omega_i\in\lam[k_i]{V^*}$, and $\omega_i=\pi(T_i)$ for some $T_i\in\lin[k_i]{V}$.
    \begin{itemize}
        \item Note that it is Theorem 1.5.13 that allows us to find $T_i$ such that $\omega_i=\pi(T_i)$.
    \end{itemize}
    \item Claim 1.6.3: The wedge product is well-defined, i.e., it does not depend on our choices of $T_i$.
    \begin{proof}
        We prove WLOG that $\wedge$ is well defined with respect to $T_1$. Suppose $\omega_1=\pi(T_1)=\pi(T_1')$. Then by the definition of the quotient map, $T_1'=T_1+W_1$ for some $W_1\in\ide[k_1]{V}$. But this means that
        \begin{equation*}
            T_1'\otimes T_2 = (T_1+W_1)\otimes T_2
            = T_1\otimes T_2+W_1\otimes T_2
        \end{equation*}
        where $W_1\otimes T_2\in\ide[k_1+k_2]{V}$ by Proposition 1.5.3. It follows that
        \begin{equation*}
            \pi(T_1'\otimes T_2) = \pi(T_1\otimes T_2)
        \end{equation*}
    \end{proof}
    \item The wedge product also generalizes to higher orders, obeying associativity, scalar multiplication, and distributivity.
    \item \textbf{Decomposable element} (of $\lam[k]{V^*}$): An element of $\lam[k]{V^*}$ of the form $\ell_1\wedge\cdots\wedge\ell_k$ where $\ell_1,\dots,\ell_k\in V^*$.
    \item Claim 1.6.8: The following wedge product identity holds for decomposable elements of $\lam[k]{V^*}$.
    \begin{equation*}
        \ell_{\sigma(1)}\wedge\cdots\wedge\ell_{\sigma(k)} = (-1)^\sigma\ell_1\wedge\cdots\wedge\ell_k
    \end{equation*}
    \begin{proof}
        Let $T=\ell_1\otimes\cdots\otimes\ell_k$. It follows by Proposition 1.4.14(1) that $T^\sigma=\ell_{\sigma(1)}\otimes\cdots\otimes\ell_{\sigma(k)}$. Therefore, we have that
        \begin{align*}
            \ell_{\sigma(1)}\wedge\cdots\wedge\ell_{\sigma(k)} &= \pi(\ell_{\sigma(1)}\otimes\cdots\otimes\ell_{\sigma(k)})\\
            &= \pi(T^\sigma)\\
            &= \pi[(-1)^\sigma T+W]\\
            &= (-1)^\sigma\pi(T)\\
            &= (-1)^\sigma\pi(\ell_1\otimes\cdots\otimes\ell_k)\\
            &= (-1)^\sigma\ell_1\wedge\cdots\wedge\ell_k
        \end{align*}
        as desired.
    \end{proof}
    \item An important consequence of Claim 1.6.8 is that
    \begin{equation*}
        \ell_1\wedge\ell_2 = -\ell_2\wedge\ell_1
    \end{equation*}
    \item Theorem 1.6.10: If $\omega_1\in\lam[r]{V^*}$ and $\omega_2\in\lam[s]{V^*}$, then
    \begin{equation*}
        \omega_1\wedge\omega_2 = (-1)^{rs}\omega_2\wedge\omega_1
    \end{equation*}
    \begin{itemize}
        \item This can be deduced from Claim 1.6.8.
        \item Hint: It suffices to prove this for decomposable elements, i.e., for $\omega_1=\ell_1\wedge\cdots\wedge\ell_r$ and $\omega_2=\ell_1'\wedge\cdots\wedge\ell_s'$.
    \end{itemize}
    \item Theorem 1.6.13: The elements
    \begin{equation*}
        e_{i_1}^*\wedge\cdots\wedge e_{i_k}^* = \pi(e_I^*) = \pi(e_{i_1}^*\otimes\cdots\otimes e_{i_k}^*)
    \end{equation*}
    with $I$ strictly increasing are basis vectors of $\lam[k]{V^*}$.
    \begin{proof}
        Follows from the facts that the $\psi_I$ for $I$ strictly increasing constitute a basis of $\alt[k]{V}$ by Proposition 1.4.26 and $\pi$ is an isomorphism $\alt[k]{V}\to\lam[k]{V^*}$.
    \end{proof}
    \item $\bm{\iota_vT}$: The $(k-1)$-tensor defined by
    \begin{equation*}
        (\iota_vT)(v_1,\dots,v_{k-1}) = \sum_{r=1}^k(-1)^{r-1}T(v_1,\dots,v_{r-1},v,v_r,\dots,v_{k-1})
    \end{equation*}
    where $T\in\lin[k]{V}$, $k\in\N_0$, $V$ is a vector space, and $v\in V$.
    \item If $v=v_1+v_2$, then
    \begin{equation*}
        \iota_vT = \iota_{v_1}T+\iota_{v_2}T
    \end{equation*}
    \item If $T=T_1+T_2$, then
    \begin{equation*}
        \iota_vT = \iota_vT_1+\iota_vT_2
    \end{equation*}
    \item Lemma 1.7.4: If $T=\ell_1\otimes\cdots\otimes\ell_k$, then
    \begin{equation*}
        \iota_vT = \sum_{r=1}^k(-1)^{r-1}\ell_r(v)\ell_1\otimes\cdots\otimes\hat{\ell}_r\otimes\cdots\otimes\ell_k
    \end{equation*}
    where the hat over $\ell_r$ means that $\ell_r$ is deleted from the tensor product.
    \item Lemma 1.7.6: $T_1\in\lin[p]{V}$ and $T_2\in\lin[q]{V}$ imply
    \begin{equation*}
        \iota_v(T_1\otimes T_2) = \iota_vT_1\otimes T_2+(-1)^pT_1\otimes\iota_vT_2
    \end{equation*}
    \item Lemma 1.7.8: $T\in\lin[k]{V}$ implies that for all $v\in V$, we have
    \begin{equation*}
        \iota_v(\iota_vT) = 0
    \end{equation*}
    \begin{proof}
        It suffices by linearity to prove this for decomposable tensors. We induct on $k$. For the base case $k=1$, the claim is trivially true. Now suppose inductively that we have proven the claim for $k-1$. Consider $\ell_1\otimes\cdots\otimes\ell_k$. Taking $T=\ell_1\otimes\cdots\otimes\ell_{k-1}$ and $\ell=\ell_k$, we obtain
        \begin{equation*}
            \iota_v(\iota_v(T\otimes\ell)) = \iota_v(\iota_vT)\otimes\ell+(-1)^{k-2}\ell(v)\iota_vT+(-1)^{k-1}\ell(v)\iota_vT
        \end{equation*}
        The first term is zero by the inductive hypothesis, and the second two cancel each other out, as desired.
    \end{proof}
    \item Claim 1.7.10: For all $v_1,v_2\in V$, we have that
    \begin{equation*}
        \iota_{v_1}\iota_{v_2} = -\iota_{v_2}\iota_{v_1}
    \end{equation*}
    \begin{proof}
        Let $v=v_1+v_2$. Then $\iota_v=\iota_{v_1}+\iota_{v_2}$. Therefore,
        \begin{align*}
            0 &= \iota_v\iota_v\tag*{Lemma 1.7.8}\\
            &= (\iota_{v_1}+\iota_{v_2})(\iota_{v_1}+\iota_{v_2})\\
            &= \iota_{v_1}\iota_{v_1}+\iota_{v_1}\iota_{v_2}+\iota_{v_2}\iota_{v_1}+\iota_{v_2}\iota_{v_2}\\
            &= \iota_{v_1}\iota_{v_2}+\iota_{v_2}\iota_{v_1}\tag*{Lemma 1.7.8}
        \end{align*}
        yielding the desired result.
    \end{proof}
    \item Lemma 1.7.11: If $T\in\lin[k]{V}$ is redundant, then so is $\iota_vT$.
    \begin{proof}
        Let $T=T_1\otimes\ell\otimes\ell\otimes T_2$ where $\ell\in V^*$, $T_1\in\lin[p]{V}$, and $T_2\in\lin[q]{V}$. By Lemma 1.7.6, we have that
        \begin{equation*}
            \iota_vT = \iota_vT_1\otimes\ell\otimes\ell\otimes T_2+(-1)^pT_1\otimes\iota_v(\ell\otimes\ell)\otimes T_2+(-1)^{p+2}T_1\otimes\ell\otimes\ell\otimes\iota_vT_2
        \end{equation*}
        Thus, since the first and third terms above are redundant and $\iota_v(\ell\otimes\ell)=\ell(v)\ell-\ell(v)\ell=0$ by Lemma 1.7.4, we have the desired result.
    \end{proof}
    \item $\bm{\iota_v\omega}$: The $\ide[k]{V}$-coset $\pi(\iota_vT)$, where $\omega=\pi(T)$.
    \item Proves that $\iota_v\omega$ does not depend on the choice of $T$.
    \item \textbf{Inner product operation}: The linear map $\iota_v:\lam[k]{V^*}\to\lam[k-1]{V^*}$.
    \item The inner product has the following important identities.
    \begin{align*}
        \iota_{(v_1+v_2)}\omega &= \iota_{v_1}\omega+\iota_{v_2}\omega\\
        \iota_v(\omega_1\wedge\omega_2) &= \iota_v\omega_1\wedge\omega_2+(-1)^p\omega_1\wedge\omega_2\\
        \iota_v(\iota_v\omega) &= 0\\
        \iota_{v_1}\iota_{v_2}\omega &= -\iota_{v_2}\iota_{v_1}\omega
    \end{align*}
\end{itemize}




\end{document}